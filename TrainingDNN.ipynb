{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fbb322",
   "metadata": {},
   "source": [
    "# Implementing Batch Normalisation with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f0c9e",
   "metadata": {},
   "source": [
    "## Load the dataset and create training set, test set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781c550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 08:01:41.807574: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-07 08:01:41.808623: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# load the dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# scale \n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# split for validation set\n",
    "X_valid, X_train = X_train_full[: 5000], X_train_full[5000: ]\n",
    "y_valid, y_train = y_train_full[: 5000], y_train_full[5000: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b7d5dc",
   "metadata": {},
   "source": [
    "## create the model with BN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f278ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 13:29:00.155920: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-10-15 13:29:00.156064: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-15 13:29:00.156173: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-8E5U3B3): /proc/driver/nvidia/version does not exist\n",
      "2021-10-15 13:29:00.167935: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "import numpy as np\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# create the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef591464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650659f",
   "metadata": {},
   "source": [
    "## check the parameters of the first BN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625787f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c56a3c",
   "metadata": {},
   "source": [
    "## add BN layers before activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5873770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6922e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7cfa27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 19:18:33.271764: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.9161 - accuracy: 0.6988 - val_loss: 0.6329 - val_accuracy: 0.7954\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6370 - accuracy: 0.7866 - val_loss: 0.5427 - val_accuracy: 0.8192\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5747 - accuracy: 0.8044 - val_loss: 0.5043 - val_accuracy: 0.8324\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.5388 - accuracy: 0.8167 - val_loss: 0.4796 - val_accuracy: 0.8394\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.5142 - accuracy: 0.8221 - val_loss: 0.4631 - val_accuracy: 0.8418\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4986 - accuracy: 0.8271 - val_loss: 0.4502 - val_accuracy: 0.8482\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4865 - accuracy: 0.8314 - val_loss: 0.4394 - val_accuracy: 0.8496\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4755 - accuracy: 0.8344 - val_loss: 0.4325 - val_accuracy: 0.8540\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4649 - accuracy: 0.8397 - val_loss: 0.4256 - val_accuracy: 0.8554\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4566 - accuracy: 0.8405 - val_loss: 0.4190 - val_accuracy: 0.8584\n"
     ]
    }
   ],
   "source": [
    "# history\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                   validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48bed76",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3627a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0, learning_rate=1e-3)\n",
    "\n",
    "# compile the model with the defined optimizer\n",
    "model.compile(loss=\"mse\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8e7ee",
   "metadata": {},
   "source": [
    "## Reusing Pre-Trained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239e70b",
   "metadata": {},
   "source": [
    "### split the MNIST dataset into dataset A and dataset B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ac73746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for splitting data\n",
    "def split_dataset(X, y):\n",
    "    \n",
    "    # create labels y_A for dataset A\n",
    "    ## find the condition that the label corresponds to sandals or shirts\n",
    "    y_5_or_6 = (y == 5) | (y == 6)\n",
    "    \n",
    "    ## set all the non-sandal and non-shirt labels together for dataset A\n",
    "    y_A = y[~y_5_or_6]\n",
    "    \n",
    "    ## the original labels 7, 8, 9 should be set to 5, 6, 7 respectively\n",
    "    y_A[y_A > 6] -= 2\n",
    "    \n",
    "    # create labels y_B for dataset B\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32)\n",
    "    \n",
    "    return ((X[~y_5_or_6], y_A), \n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "# split the training set into A and B\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "\n",
    "# split the validation set into A and B\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "\n",
    "# split the test set into A and B\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "\n",
    "# assign the first 200 instances of training set B for the task\n",
    "X_train_B = X_train_B[: 200]\n",
    "y_train_B = y_train_B[: 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "031864f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ec9eb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0171d",
   "metadata": {},
   "source": [
    "### create, compile and history the model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fea5c1ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 4s 2ms/step - loss: 0.5422 - accuracy: 0.8223 - val_loss: 0.3624 - val_accuracy: 0.8777\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3374 - accuracy: 0.8833 - val_loss: 0.3084 - val_accuracy: 0.8956\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3032 - accuracy: 0.8951 - val_loss: 0.2824 - val_accuracy: 0.9033\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2848 - accuracy: 0.9027 - val_loss: 0.2703 - val_accuracy: 0.9098\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2723 - accuracy: 0.9058 - val_loss: 0.2612 - val_accuracy: 0.9111\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2632 - accuracy: 0.9087 - val_loss: 0.2579 - val_accuracy: 0.9136\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2556 - accuracy: 0.9119 - val_loss: 0.2533 - val_accuracy: 0.9121\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2493 - accuracy: 0.9150 - val_loss: 0.2455 - val_accuracy: 0.9183\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2443 - accuracy: 0.9162 - val_loss: 0.2424 - val_accuracy: 0.9188\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2397 - accuracy: 0.9178 - val_loss: 0.2417 - val_accuracy: 0.9170\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2356 - accuracy: 0.9194 - val_loss: 0.2361 - val_accuracy: 0.9185\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2315 - accuracy: 0.9210 - val_loss: 0.2394 - val_accuracy: 0.9163\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2281 - accuracy: 0.9211 - val_loss: 0.2313 - val_accuracy: 0.9200\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2247 - accuracy: 0.9223 - val_loss: 0.2296 - val_accuracy: 0.9223\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2218 - accuracy: 0.9235 - val_loss: 0.2336 - val_accuracy: 0.9155\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2187 - accuracy: 0.9252 - val_loss: 0.2292 - val_accuracy: 0.9220\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2163 - accuracy: 0.9249 - val_loss: 0.2331 - val_accuracy: 0.9185\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2132 - accuracy: 0.9260 - val_loss: 0.2349 - val_accuracy: 0.9150\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2106 - accuracy: 0.9270 - val_loss: 0.2253 - val_accuracy: 0.9195\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2085 - accuracy: 0.9284 - val_loss: 0.2223 - val_accuracy: 0.9213\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# initialise the model A \n",
    "model_A = keras.models.Sequential()\n",
    "\n",
    "# add layers on model A\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "# compile the model\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "# history fit\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                     validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf8627",
   "metadata": {},
   "source": [
    "### save the model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b1a55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd8c97",
   "metadata": {},
   "source": [
    "### create, compile, and history-fit model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09455ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 28ms/step - loss: 0.7163 - accuracy: 0.5300 - val_loss: 0.5416 - val_accuracy: 0.7343\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.4794 - accuracy: 0.8000 - val_loss: 0.4450 - val_accuracy: 0.8580\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.3985 - accuracy: 0.8800 - val_loss: 0.3907 - val_accuracy: 0.8905\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3490 - accuracy: 0.9100 - val_loss: 0.3515 - val_accuracy: 0.9006\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.3112 - accuracy: 0.9150 - val_loss: 0.3198 - val_accuracy: 0.9128\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2797 - accuracy: 0.9250 - val_loss: 0.2945 - val_accuracy: 0.9219\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2546 - accuracy: 0.9400 - val_loss: 0.2748 - val_accuracy: 0.9270\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2347 - accuracy: 0.9450 - val_loss: 0.2566 - val_accuracy: 0.9310\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2168 - accuracy: 0.9450 - val_loss: 0.2399 - val_accuracy: 0.9381\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2007 - accuracy: 0.9550 - val_loss: 0.2270 - val_accuracy: 0.9381\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1879 - accuracy: 0.9550 - val_loss: 0.2154 - val_accuracy: 0.9422\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1765 - accuracy: 0.9650 - val_loss: 0.2049 - val_accuracy: 0.9442\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1660 - accuracy: 0.9750 - val_loss: 0.1954 - val_accuracy: 0.9473\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1569 - accuracy: 0.9750 - val_loss: 0.1865 - val_accuracy: 0.9513\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1482 - accuracy: 0.9750 - val_loss: 0.1783 - val_accuracy: 0.9503\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1405 - accuracy: 0.9800 - val_loss: 0.1711 - val_accuracy: 0.9513\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1334 - accuracy: 0.9850 - val_loss: 0.1645 - val_accuracy: 0.9564\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1270 - accuracy: 0.9850 - val_loss: 0.1588 - val_accuracy: 0.9584\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1214 - accuracy: 0.9850 - val_loss: 0.1536 - val_accuracy: 0.9594\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1161 - accuracy: 0.9850 - val_loss: 0.1487 - val_accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "# initialise model B\n",
    "model_B = keras.models.Sequential()\n",
    "\n",
    "# add input layer to model B\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "# add hidden layer to model B\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "    \n",
    "# add output layer\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# compile the model B\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "# hitory fit\n",
    "history_B = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                       validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a2c75b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# show the summary of B\n",
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f5c36",
   "metadata": {},
   "source": [
    "### create  Model B_on_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "335aa24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1216 - accuracy: 0.3850 - val_loss: 1.0541 - val_accuracy: 0.4493\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.0192 - accuracy: 0.4400 - val_loss: 0.9636 - val_accuracy: 0.5030\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9287 - accuracy: 0.5100 - val_loss: 0.8840 - val_accuracy: 0.5335\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8491 - accuracy: 0.5650 - val_loss: 0.8113 - val_accuracy: 0.5832\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5939 - accuracy: 0.7050 - val_loss: 0.4185 - val_accuracy: 0.8174\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.3365 - accuracy: 0.8550 - val_loss: 0.2956 - val_accuracy: 0.9047\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2428 - accuracy: 0.9300 - val_loss: 0.2347 - val_accuracy: 0.9331\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1921 - accuracy: 0.9500 - val_loss: 0.1937 - val_accuracy: 0.9523\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1579 - accuracy: 0.9650 - val_loss: 0.1676 - val_accuracy: 0.9615\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1354 - accuracy: 0.9700 - val_loss: 0.1485 - val_accuracy: 0.9706\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1189 - accuracy: 0.9750 - val_loss: 0.1338 - val_accuracy: 0.9767\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1061 - accuracy: 0.9800 - val_loss: 0.1215 - val_accuracy: 0.9777\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0953 - accuracy: 0.9850 - val_loss: 0.1117 - val_accuracy: 0.9828\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0865 - accuracy: 0.9850 - val_loss: 0.1037 - val_accuracy: 0.9868\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0794 - accuracy: 0.9950 - val_loss: 0.0960 - val_accuracy: 0.9899\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0725 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 0.9909\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0672 - accuracy: 1.0000 - val_loss: 0.0850 - val_accuracy: 0.9909\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0624 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9919\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0577 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 0.9929\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0539 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9919\n"
     ]
    }
   ],
   "source": [
    "# load model A from the h5 file\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "\n",
    "# create the model B_on_A with all the A's layers except from the output layer\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[: -1])\n",
    "\n",
    "# add an output layer to model B_on_A\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# freeze all the reused layers \n",
    "for layer in model_B_on_A.layers[: -1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the layer-freezed model B_on_A\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "# 4-epoch history fit\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                          validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# unfreeze all the reused layers\n",
    "for layer in model_B_on_A.layers[: -1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# compile the layer-unfreezed model\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "# 16-epoch history fit\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                          validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fd21f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14390385150909424, 0.9704999923706055]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ba81496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0721736028790474, 0.9909999966621399]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0ca4c",
   "metadata": {},
   "source": [
    "## Regularization with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f886c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# import the lib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# load the dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# scale \n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# split for validation set\n",
    "X_valid, X_train = X_train_full[: 5000], X_train_full[5000: ]\n",
    "y_valid, y_train = y_train_full[: 5000], y_train_full[5000: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf6c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 08:17:04.415836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-07 08:17:04.417252: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-07 08:17:04.417477: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-8E5U3B3): /proc/driver/nvidia/version does not exist\n",
      "2021-11-07 08:17:04.454818: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-07 08:17:06.663784: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 7s 3ms/step - loss: 0.5737 - accuracy: 0.8027 - val_loss: 0.3632 - val_accuracy: 0.8706\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4200 - accuracy: 0.8463 - val_loss: 0.3412 - val_accuracy: 0.8740\n"
     ]
    }
   ],
   "source": [
    "# create the model \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    # add Dropout layer in front of all the hidden layers\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    \n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    \n",
    "    # add Dropout layer in front of the output layer\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# scale the training instances to mean 0 and standard deviation 1\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# obtain the history\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                   validation_data=(X_valid_scaled, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
