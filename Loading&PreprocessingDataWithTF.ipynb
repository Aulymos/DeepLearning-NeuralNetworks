{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7764eb03",
   "metadata": {},
   "source": [
    "# Section 1: Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcdffb",
   "metadata": {},
   "source": [
    "## a dataset: a sequence of data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ad94e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 14:30:59.327525: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-12 14:30:59.327627: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a385b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# create a tf.data.Dataset\n",
    "\n",
    "## create a (10, ) type tensor \n",
    "X = tf.range(10)\n",
    "\n",
    "## create a tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "## iterate the dataset to have a look at the generated data items\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26923ca",
   "metadata": {},
   "source": [
    "## Section 1.1: Chaining Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f1dd3",
   "metadata": {},
   "source": [
    "### repeat() and batch() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f835c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "\n",
    "dataset = dataset.repeat(3).batch(7, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78fa6ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# show the items\n",
    "\n",
    "for e in dataset:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d479a4e",
   "metadata": {},
   "source": [
    "### map() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d055a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e92ab4",
   "metadata": {},
   "source": [
    "### apply(tf.data.experimental.function()) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74b0a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.unbatch())\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e751c",
   "metadata": {},
   "source": [
    "### filter() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89de384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.range(10)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6cf978a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b45a06b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# just look at a few items \n",
    "\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93cf638",
   "metadata": {},
   "source": [
    "## Section 1.2 Shuffling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b01596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 17:34:31.280870: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-16 17:34:31.281266: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99dcc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 17:34:42.451154: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-16 17:34:42.452182: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-16 17:34:42.452296: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-8E5U3B3): /proc/driver/nvidia/version does not exist\n",
      "2021-12-16 17:34:42.462851: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae7db89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# show the items\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8271ae6f",
   "metadata": {},
   "source": [
    "### split the California dataset to multiple CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171efe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libs\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b481343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951e5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data and labels\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1),\n",
    "                                                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d277ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, validation and test sets\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f53acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the mean and std of training data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean, X_std = scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274b7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for splitting the data into multiple files (20 files in this example)\n",
    "\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    \n",
    "    # initialise the path of the directory\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    # create the directory\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    # initialise the name format of the path of the particular file\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "    \n",
    "    # initialise the path data structure with an empty list\n",
    "    filepaths = []\n",
    "    \n",
    "    # obtain the number of data instances\n",
    "    m = len(data)\n",
    "    # iterate the file index and rows in each file\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        # assign the particular path for each file\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        # write file\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    return filepaths       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11c3df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the generated files by running the function\n",
    "\n",
    "# create the training, validation and test data np.array \n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "\n",
    "# obtain the headers\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "# obtain the path name of the header\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "# run the defined function \n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f4ac857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/housing/my_train_00.csv',\n",
       " 'datasets/housing/my_train_01.csv',\n",
       " 'datasets/housing/my_train_02.csv',\n",
       " 'datasets/housing/my_train_03.csv',\n",
       " 'datasets/housing/my_train_04.csv',\n",
       " 'datasets/housing/my_train_05.csv',\n",
       " 'datasets/housing/my_train_06.csv',\n",
       " 'datasets/housing/my_train_07.csv',\n",
       " 'datasets/housing/my_train_08.csv',\n",
       " 'datasets/housing/my_train_09.csv',\n",
       " 'datasets/housing/my_train_10.csv',\n",
       " 'datasets/housing/my_train_11.csv',\n",
       " 'datasets/housing/my_train_12.csv',\n",
       " 'datasets/housing/my_train_13.csv',\n",
       " 'datasets/housing/my_train_14.csv',\n",
       " 'datasets/housing/my_train_15.csv',\n",
       " 'datasets/housing/my_train_16.csv',\n",
       " 'datasets/housing/my_train_17.csv',\n",
       " 'datasets/housing/my_train_18.csv',\n",
       " 'datasets/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the train file paths\n",
    "\n",
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "222778a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the content of one particular csv file\n",
    "\n",
    "with open(\"datasets/housing/my_train_00.csv\", \"r\") as file:\n",
    "    for i in range(5):\n",
    "        print(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894ee3a",
   "metadata": {},
   "source": [
    "###  Build the Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e9e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset of shuffled file paths  \n",
    "\n",
    "filepaths_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d9e4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new string dataset consisting of five data instances where each is extracted from a filepath\n",
    "\n",
    "n_readers = 5\n",
    "dataset = filepaths_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea1422d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n",
      "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n"
     ]
    }
   ],
   "source": [
    "# take a look at the dataset content\n",
    "\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55e0c2",
   "metadata": {},
   "source": [
    "## Section 1.3 Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b892555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data preprocessing function\n",
    "\n",
    "## assign the number of inputs\n",
    "n_inputs = X_train.shape[-1]\n",
    "\n",
    "## function definition\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    \n",
    "    # assign the defaults of input instance type \n",
    "    defs = [0.0] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    \n",
    "    # assign the field with decoding the csv file \n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    \n",
    "    # assign the training data\n",
    "    X = tf.stack(fields[: -1])\n",
    "    y = tf.stack(fields[-1: ])\n",
    "    \n",
    "    return (X - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af530f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the preprocessing result for 1 instance\n",
    "\n",
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c7a8f",
   "metadata": {},
   "source": [
    "## Section 1.4 Putting Everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b045d9",
   "metadata": {},
   "source": [
    "#### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91388de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libs \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# load the dataset\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef48645",
   "metadata": {},
   "source": [
    "#### training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50e89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full training and test sets\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1),\n",
    "                                                              random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e94bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split full training set into training and validation sets\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6585f81",
   "metadata": {},
   "source": [
    "#### multiple CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ecc469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for splitting the data into multiple files (20 files in this example)\n",
    "\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    \n",
    "    # initialise the path of the directory\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    # create the directory\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    # initialise the name format of the path of the particular file\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "    \n",
    "    # initialise the path data structure with an empty list\n",
    "    filepaths = []\n",
    "    \n",
    "    # obtain the number of data instances\n",
    "    m = len(data)\n",
    "    # iterate the file index and rows in each file\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        # assign the particular path for each file\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        # write file\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    return filepaths       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed15b0e",
   "metadata": {},
   "source": [
    "#### preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a87f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 09:20:45.665650: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-18 09:20:45.665769: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# define the data preprocessing function\n",
    "\n",
    "## import the lib\n",
    "import tensorflow as tf\n",
    "\n",
    "## obtain the mean and standard deviation of traning data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean, X_std = scaler.mean_, scaler.scale_\n",
    "\n",
    "## assign the number of inputs\n",
    "n_inputs = X_train.shape[-1]\n",
    "\n",
    "## function definition\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    \n",
    "    # assign the defaults of input instance type \n",
    "    defs = [0.0] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    \n",
    "    # assign the field with decoding the csv file \n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    \n",
    "    # assign the training data\n",
    "    X = tf.stack(fields[: -1])\n",
    "    y = tf.stack(fields[-1: ])\n",
    "    \n",
    "    return (X - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a16a00",
   "metadata": {},
   "source": [
    "#### shuffled dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1b3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the splitted file paths by running the defined function\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003c6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_repeats=1, n_readers=5,\n",
    "                      n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                      n_parse_threads=5, batch_size=32):\n",
    "    \n",
    "    # obtain the file list dataset\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(n_repeats)\n",
    "    \n",
    "    # obtain the extracted text string dataset\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    \n",
    "    # shuffle the dataset\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    # map the dataset with preprocess function\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    \n",
    "    # batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed669a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 09:21:00.231493: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-18 09:21:00.232114: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-18 09:21:00.232351: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-8E5U3B3): /proc/driver/nvidia/version does not exist\n",
      "2021-12-18 09:21:00.237298: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, n_repeats=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0df99",
   "metadata": {},
   "source": [
    "### 1.5 Using the Dataset with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398c7efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 2s 4ms/step - loss: 1.6294 - val_loss: 0.8443\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.7159 - val_loss: 0.6583\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.6615 - val_loss: 0.5997\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5864 - val_loss: 0.5632\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5473 - val_loss: 0.5960\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5573 - val_loss: 0.5158\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5392 - val_loss: 0.4954\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5023 - val_loss: 0.4800\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5070 - val_loss: 0.4906\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4879 - val_loss: 0.4556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08c0208e20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the lib\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# specify the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6393286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4775973856449127"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "model.evaluate(test_set, steps=len(X_test)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb31e14",
   "metadata": {},
   "source": [
    "# Section 2 TFRecord Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e935e",
   "metadata": {},
   "source": [
    "### create a TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bfd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as file:\n",
    "    file.write(b\"This is the first record\")\n",
    "    file.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "759228f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# read the file\n",
    "\n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92daf8c",
   "metadata": {},
   "source": [
    "## 2.1 Compressed TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45dbbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a compressed TFRecord file\n",
    "\n",
    "## set the option\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "\n",
    "## write the file\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as file:\n",
    "    file.write(b\"This is the first record\")\n",
    "    file.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d420050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# read the compressed file\n",
    "\n",
    "## dataset assignment\n",
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"], \n",
    "                                  compression_type=\"GZIP\")\n",
    "\n",
    "## read \n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b68bb8",
   "metadata": {},
   "source": [
    "# Section 3: The Feature API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e071375",
   "metadata": {},
   "source": [
    "### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a98423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libs \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# load the dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# full training and test sets\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1),\n",
    "                                                              random_state=42)\n",
    "# split full training set into training and validation sets\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015920b5",
   "metadata": {},
   "source": [
    "### mean and standard deviation of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ea3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtain the mean and standard deviation of traning data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean, X_std = scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4d518",
   "metadata": {},
   "source": [
    "### tf.feature_column package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38449d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 10:48:45.197824: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-03 10:48:45.198717: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "import tensorflow as tf\n",
    "\n",
    "# obtain the mean and standard deviation \n",
    "age_mean, age_std = X_mean[1], X_std[1]\n",
    "\n",
    "# define a categorical feature\n",
    "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\",\n",
    "                                                     normalizer_fn=lambda x: (x - age_mean) / age_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee159dd6",
   "metadata": {},
   "source": [
    "### bucketize income feature into categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624a48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the numeric column of median_income\n",
    "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
    "\n",
    "# bucketize the feature\n",
    "bucketized_income = tf.feature_column.bucketized_column(median_income,\n",
    "                                                       boundaries=[1.5, 3.0, 4.5, 6.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f92315",
   "metadata": {},
   "source": [
    "## 3.1 Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb9668",
   "metadata": {},
   "source": [
    "### example: ocean_proximity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39947f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the vocabulary list\n",
    "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "\n",
    "# define categorical feature with vocabulary list \n",
    "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\"ocean_proximity\", \n",
    "                                                                            ocean_prox_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0e5c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VocabularyListCategoricalColumn(key='ocean_proximity', vocabulary_list=('<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'), dtype=tf.string, default_value=-1, num_oov_buckets=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocean_proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d9a3b",
   "metadata": {},
   "source": [
    "### hash bucket for large vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b652d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_hash = tf.feature_column.categorical_column_with_hash_bucket(\"city\", hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057f415",
   "metadata": {},
   "source": [
    "## 3.2 Crossed Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f001068",
   "metadata": {},
   "source": [
    "### example 1: cross bucketized feature with another feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea058ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketize the housing_median_age feature\n",
    "bucketized_age = tf.feature_column.bucketized_column(housing_median_age,\n",
    "                                                    boundaries=[-1.0, -0.5, 0.0, 0.5, 1.0])\n",
    "\n",
    "# cross the bucketized feature with ocean_proximity\n",
    "age_and_ocean_proximity = tf.feature_column.crossed_column([bucketized_age, ocean_proximity],\n",
    "                                                          hash_bucket_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517aec8",
   "metadata": {},
   "source": [
    "### example 2: cross latitude and longitude into a single categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f96941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the latitude and longitude numeric features\n",
    "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
    "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
    "\n",
    "# bucketize the features\n",
    "bucketized_latitude = tf.feature_column.bucketized_column(latitude, \n",
    "                                                         boundaries=list(np.linspace(32.0, 42.0, 20 - 1)))\n",
    "bucketized_longitude = tf.feature_column.bucketized_column(longitude, \n",
    "                                                           boundaries=list(np.linspace(-125.0, -114.0, 20 - 1)))\n",
    "\n",
    "# cross the bucketized features together\n",
    "location = tf.feature_column.crossed_column([bucketized_latitude, bucketized_longitude], \n",
    "                                           hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3383a",
   "metadata": {},
   "source": [
    "## 3.3 Encoding Catergorical Features: One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579ec00",
   "metadata": {},
   "source": [
    "### tf.feature_column.indicator_column() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26937f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the categorical feature for filling into the method\n",
    "\n",
    "## assign the vocabulary list\n",
    "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "\n",
    "## define categorical feature with vocabulary list \n",
    "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\"ocean_proximity\", \n",
    "                                                                            ocean_prox_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa72c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot vector for ocean_proximity\n",
    "\n",
    "ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bceb65f",
   "metadata": {},
   "source": [
    "## 3.4 Encoding Categorical Features: Using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5bbb6",
   "metadata": {},
   "source": [
    "### tf.feature_column.embedding_column() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194dca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the categorical feature for filling into the method\n",
    "\n",
    "## assign the vocabulary list\n",
    "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "\n",
    "## define categorical feature with vocabulary list\n",
    "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\"ocean_proximity\", \n",
    "                                                                           ocean_prox_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3257ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D embedding\n",
    "\n",
    "ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\n",
    "                                                          dimension=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23fb3c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='ocean_proximity', vocabulary_list=('<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=2, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7fc610b15280>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)\n"
     ]
    }
   ],
   "source": [
    "# display the embeded feature\n",
    "print(ocean_proximity_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6e2dd",
   "metadata": {},
   "source": [
    "## 3.5 Parsing: Using Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d131b8e",
   "metadata": {},
   "source": [
    "### parse all the feature columns to generate feature descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5e2af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import Feature, Features, Example\n",
    "from tensorflow.train import BytesList, FloatList, Int64List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "445fdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numeric feature column for the target \"median_house_value\"\n",
    "\n",
    "median_house_value = tf.feature_column.numeric_column(\"median_house_value\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9003d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature columns with input features and target \n",
    "columns = [housing_median_age, median_house_value]\n",
    "\n",
    "# generate feature descriptions\n",
    "feature_descriptions = tf.feature_column.make_parse_example_spec(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7416b091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'housing_median_age': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None),\n",
       " 'median_house_value': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the feature description\n",
    "\n",
    "feature_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "409673d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TFRecord file\n",
    "\n",
    "with tf.io.TFRecordWriter(\"my_data_with_features.tfrecords\") as f:\n",
    "    for x, y in zip(X_train[:, 1:2], y_train):\n",
    "        example = Example(features=Features(feature={\n",
    "            \"housing_median_age\": Feature(float_list=FloatList(value=[x])),\n",
    "            \"median_house_value\": Feature(float_list=FloatList(value=[y]))\n",
    "        }))\n",
    "        f.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56680bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that parses serialized examples and separates the target column from the input features\n",
    "\n",
    "def parse_examples(serialized_examples):\n",
    "    \n",
    "    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n",
    "    targets = examples.pop(\"median_house_value\")\n",
    "    \n",
    "    return examples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f2168b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 13:59:10.936001: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-03 13:59:10.937612: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-03 13:59:10.937871: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-8E5U3B3): /proc/driver/nvidia/version does not exist\n",
      "2022-02-03 13:59:10.948051: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# load the dataset from a TFRecord file\n",
    "\n",
    "batch_size = 32\n",
    "dataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\n",
    "dataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5ade01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the input feature column to be applied as the feature columns for layer initialization\n",
    "\n",
    "columns_without_target = columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2916efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'housing_median_age': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float32>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'housing_median_age': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float32>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "362/362 [==============================] - 1s 1ms/step - loss: 3.6557 - accuracy: 0.0010\n",
      "Epoch 2/5\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.8862 - accuracy: 0.0035\n",
      "Epoch 3/5\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.4492 - accuracy: 0.0027\n",
      "Epoch 4/5\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.3616 - accuracy: 0.0028\n",
      "Epoch 5/5\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.3276 - accuracy: 0.0029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc60e8e21f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model creation, fitting and compiling\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.DenseFeatures(feature_columns=columns_without_target),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\",\n",
    "             optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(dataset, steps_per_epoch=len(X_train)//batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aabee3",
   "metadata": {},
   "source": [
    "# Section 4 TF Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c516a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
