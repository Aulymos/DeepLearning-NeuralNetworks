{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e91253",
   "metadata": {},
   "source": [
    "# Chapter 10 Neural Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c1f54",
   "metadata": {},
   "source": [
    "## Section 1: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca70420c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107/2318198134.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = (iris.target == 0).astype(np.int) # label: satosa: 1, not-satosa: 0\n"
     ]
    }
   ],
   "source": [
    "# import the libs\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# load the data\n",
    "iris = load_iris()\n",
    "\n",
    "# create the sample data instances\n",
    "X = iris.data[:, (2, 3)]    # pedal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # label: satosa: 1, not-satosa: 0\n",
    "\n",
    "# create the Perceptron model\n",
    "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# fit \n",
    "per_clf.fit(X, y)\n",
    "\n",
    "# predict a new instance\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da8054",
   "metadata": {},
   "source": [
    "## Section 2: Implementing MLPs with TensorFlow Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f59ca",
   "metadata": {},
   "source": [
    "### 2.1 check tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f533c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-11 12:41:59.242368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-11 12:41:59.243243: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the libs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a07992e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ff1124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ababd2c1",
   "metadata": {},
   "source": [
    "### 2.2 building an Image classifier using the sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f65642",
   "metadata": {},
   "source": [
    "### using Keras to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9547ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 19:50:47.164336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-14 19:50:47.164467: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# load the dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "# split the dataset into training set and test set\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2402e",
   "metadata": {},
   "source": [
    "### create validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c155913",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[: 5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "\n",
    "y_valid, y_train = y_train_full[: 5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac7187",
   "metadata": {},
   "source": [
    "### create label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1ca60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-Shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb596637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f344663",
   "metadata": {},
   "source": [
    "### create the model using the sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96f58a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 19:51:13.724520: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-14 19:51:13.725178: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-14 19:51:13.725462: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-8E5U3B3): /proc/driver/nvidia/version does not exist\n",
      "2021-09-14 19:51:13.729034: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# create sequential model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# add the flatten layer to preprocess\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "# add the 300-neuron hidden layer with ReLU activation function\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "\n",
    "# add the 100-neuron hidden layer with ReLu\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "\n",
    "# add the output layer with softmax\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d7eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# show the model's summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78eb18c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the image of the model\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4bd84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Flatten at 0x7ff13aa0d850>,\n",
       " <keras.layers.core.Dense at 0x7ff13aa0d100>,\n",
       " <keras.layers.core.Dense at 0x7ff120613a30>,\n",
       " <keras.layers.core.Dense at 0x7ff120600730>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the layers data strucutre\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "495e2d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten\n",
      "dense\n",
      "dense_1\n",
      "dense_2\n"
     ]
    }
   ],
   "source": [
    "# index of the layers\n",
    "\n",
    "for i in range (len(model.layers)):\n",
    "    print (model.layers[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1107e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07204932 -0.06615792  0.0679342  ... -0.04854907  0.07265185\n",
      "   0.05433644]\n",
      " [ 0.02576386  0.02234847  0.0057363  ... -0.07204002 -0.06593858\n",
      "  -0.05068243]\n",
      " [-0.0603852   0.06885725  0.06545372 ...  0.04100048 -0.05414899\n",
      "   0.04979403]\n",
      " ...\n",
      " [ 0.03748304 -0.04708242  0.0183041  ... -0.00442522  0.03698174\n",
      "  -0.04066655]\n",
      " [ 0.05734703 -0.02975544  0.05592373 ... -0.02377617 -0.03244746\n",
      "   0.06167027]\n",
      " [ 0.03807508  0.03841769  0.05780253 ...  0.02778498 -0.04034283\n",
      "   0.03110434]]\n"
     ]
    }
   ],
   "source": [
    "# show the weights and biases of hidden layer number 1 (\"dense\" layer)\n",
    "\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c759be",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0f47fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "             optimizer=\"sgd\", \n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f0f17",
   "metadata": {},
   "source": [
    "### train and evaluate the model - History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab17715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 19:51:47.740125: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7354 - accuracy: 0.7608 - val_loss: 0.4997 - val_accuracy: 0.8358\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4874 - accuracy: 0.8317 - val_loss: 0.4372 - val_accuracy: 0.8564\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4427 - accuracy: 0.8439 - val_loss: 0.4262 - val_accuracy: 0.8500\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4148 - accuracy: 0.8542 - val_loss: 0.4500 - val_accuracy: 0.8402\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3945 - accuracy: 0.8611 - val_loss: 0.3822 - val_accuracy: 0.8672\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3795 - accuracy: 0.8658 - val_loss: 0.3771 - val_accuracy: 0.8698\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3647 - accuracy: 0.8706 - val_loss: 0.3662 - val_accuracy: 0.8726\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3533 - accuracy: 0.8752 - val_loss: 0.3754 - val_accuracy: 0.8684\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3424 - accuracy: 0.8778 - val_loss: 0.3475 - val_accuracy: 0.8742\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3334 - accuracy: 0.8802 - val_loss: 0.3424 - val_accuracy: 0.8750\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3248 - accuracy: 0.8847 - val_loss: 0.3646 - val_accuracy: 0.8676\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3173 - accuracy: 0.8860 - val_loss: 0.3331 - val_accuracy: 0.8814\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3102 - accuracy: 0.8883 - val_loss: 0.3518 - val_accuracy: 0.8736\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3025 - accuracy: 0.8914 - val_loss: 0.3500 - val_accuracy: 0.8716\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2969 - accuracy: 0.8935 - val_loss: 0.3357 - val_accuracy: 0.8806\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2902 - accuracy: 0.8954 - val_loss: 0.3237 - val_accuracy: 0.8820\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2853 - accuracy: 0.8968 - val_loss: 0.3200 - val_accuracy: 0.8846\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2800 - accuracy: 0.8989 - val_loss: 0.3175 - val_accuracy: 0.8820\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2738 - accuracy: 0.9005 - val_loss: 0.3231 - val_accuracy: 0.8798\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2696 - accuracy: 0.9022 - val_loss: 0.3120 - val_accuracy: 0.8886\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2649 - accuracy: 0.9044 - val_loss: 0.3172 - val_accuracy: 0.8846\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2608 - accuracy: 0.9060 - val_loss: 0.3099 - val_accuracy: 0.8888\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2558 - accuracy: 0.9079 - val_loss: 0.3056 - val_accuracy: 0.8898\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2513 - accuracy: 0.9078 - val_loss: 0.3033 - val_accuracy: 0.8884\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2471 - accuracy: 0.9108 - val_loss: 0.2996 - val_accuracy: 0.8908\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2427 - accuracy: 0.9122 - val_loss: 0.2925 - val_accuracy: 0.8930\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2394 - accuracy: 0.9135 - val_loss: 0.3021 - val_accuracy: 0.8878\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2357 - accuracy: 0.9151 - val_loss: 0.3213 - val_accuracy: 0.8806\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2322 - accuracy: 0.9160 - val_loss: 0.2968 - val_accuracy: 0.8928\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2279 - accuracy: 0.9173 - val_loss: 0.2903 - val_accuracy: 0.8916\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4925c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history is: {'loss': [0.7353982329368591, 0.4873640239238739, 0.4426966905593872, 0.41479572653770447, 0.3945297598838806, 0.37945112586021423, 0.36468881368637085, 0.3532998859882355, 0.34236690402030945, 0.3334287405014038, 0.3247893154621124, 0.3172996938228607, 0.3102158010005951, 0.3025306761264801, 0.29691195487976074, 0.29016992449760437, 0.2852831184864044, 0.27999645471572876, 0.2738039195537567, 0.2696235477924347, 0.2648516595363617, 0.26080092787742615, 0.2557997703552246, 0.2513163387775421, 0.2470988929271698, 0.24269726872444153, 0.23938633501529694, 0.23572176694869995, 0.23219142854213715, 0.22787515819072723], 'accuracy': [0.7608181834220886, 0.8316545486450195, 0.843854546546936, 0.854163646697998, 0.8611272573471069, 0.865818202495575, 0.8706363439559937, 0.8751817941665649, 0.8777818083763123, 0.8802363872528076, 0.8846545219421387, 0.8859636187553406, 0.8883273005485535, 0.8913818001747131, 0.8934727311134338, 0.8954181671142578, 0.8967999815940857, 0.8989272713661194, 0.9004727005958557, 0.9022363424301147, 0.9043818116188049, 0.9059818387031555, 0.907909095287323, 0.907800018787384, 0.9107818007469177, 0.9122363924980164, 0.9134727120399475, 0.9151090979576111, 0.9160181879997253, 0.9172909259796143], 'val_loss': [0.4996524453163147, 0.4371529817581177, 0.4261949360370636, 0.44996562600135803, 0.38221636414527893, 0.3770778477191925, 0.36615538597106934, 0.3753608763217926, 0.34748393297195435, 0.34240350127220154, 0.36457517743110657, 0.3330758810043335, 0.35177868604660034, 0.35003530979156494, 0.3356696367263794, 0.32367613911628723, 0.320008784532547, 0.31747645139694214, 0.3231382668018341, 0.31196069717407227, 0.3172459900379181, 0.30994394421577454, 0.3056071400642395, 0.30331483483314514, 0.29963770508766174, 0.2925001382827759, 0.30210819840431213, 0.3213163912296295, 0.29681429266929626, 0.2902730703353882], 'val_accuracy': [0.8357999920845032, 0.8564000129699707, 0.8500000238418579, 0.8402000069618225, 0.8672000169754028, 0.8697999715805054, 0.8726000189781189, 0.868399977684021, 0.8741999864578247, 0.875, 0.8676000237464905, 0.8813999891281128, 0.8736000061035156, 0.8715999722480774, 0.8805999755859375, 0.8820000290870667, 0.8845999836921692, 0.8820000290870667, 0.879800021648407, 0.8885999917984009, 0.8845999836921692, 0.8888000249862671, 0.8898000121116638, 0.8884000182151794, 0.8907999992370605, 0.8930000066757202, 0.8877999782562256, 0.8805999755859375, 0.892799973487854, 0.8916000127792358]}\n"
     ]
    }
   ],
   "source": [
    "# show the History class object's attribute - history\n",
    "print(\"The history is: {}\".format(history.history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f20e4fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters is: {'verbose': 1, 'epochs': 30, 'steps': 1719}\n"
     ]
    }
   ],
   "source": [
    "# show the History class object's attribute - params\n",
    "print(\"The parameters is: {}\".format(history.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078f1ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The epoch is: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "# show the History class object's attribute - epoch\n",
    "print(\"The epoch is: {}\".format(history.epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f72c729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the History class object's plotting image\n",
    "\n",
    "# import the lib \n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# plot\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3acb27",
   "metadata": {},
   "source": [
    "### Using the model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "775c13d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability prediction results are: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "The class label prediction results are: [9 2 1]\n",
      "The class labels are: ['Ankle boot' 'Pullover' 'Trouser']\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "import numpy as np\n",
    "\n",
    "# create certain instances as new ones from the test set\n",
    "X_new = X_test[: 3]\n",
    "\n",
    "# obtain the probability prediction results\n",
    "y_proba = model.predict(X_new)\n",
    "print(\"The probability prediction results are: {}\".format(y_proba.round(2)))\n",
    "\n",
    "# obtain the class label prediction results\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "print(\"The class label prediction results are: {}\".format(y_pred))\n",
    "\n",
    "# obtain the class label names \n",
    "y_pred_label = np.array(class_names)[y_pred]\n",
    "print(\"The class labels are: {}\".format(y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e84e4",
   "metadata": {},
   "source": [
    "### 2.3 Building a Regression MLP using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62b6a5",
   "metadata": {},
   "source": [
    "### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06982fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# load the dataset\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16247fd2",
   "metadata": {},
   "source": [
    "### split dataset into full training set with training set, validation set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e142768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into full training set and test set\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "\n",
    "# split the full training set into training set and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580d200",
   "metadata": {},
   "source": [
    "### preprocess the training, validation and test data - scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42d2efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scaler class object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler with training set\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform \n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc561fe",
   "metadata": {},
   "source": [
    "### Building a Regression MLP using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67dee4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan \n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 914us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 857us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 892us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 838us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 838us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 829us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 834us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 830us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 851us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 909us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 844us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 826us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 845us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 851us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 925us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 864us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 911us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 847us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 840us/step - loss: nan - val_loss: nan\n",
      "162/162 [==============================] - 0s 581us/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "from tensorflow import keras\n",
    "\n",
    "# create the Sequential API\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "\n",
    "# obtain the history of the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# evaluate the model with test set after being satisfied with the validation performance\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f3db6",
   "metadata": {},
   "source": [
    "### make predictions for new instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb10e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "# create certain new instances\n",
    "X_new = X_test[: 3]\n",
    "\n",
    "# make predictions \n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ccc1d",
   "metadata": {},
   "source": [
    "### 2.4 Building Complex Models using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e5c3b",
   "metadata": {},
   "source": [
    "### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f7759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# load the dataset\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1f4a1",
   "metadata": {},
   "source": [
    "### split the dataset into training set, test set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5bfa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into full training and test set\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "\n",
    "# split the full training set into training set and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091adb5",
   "metadata": {},
   "source": [
    "### preprocess the training, validation and test sets - scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf3a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f6ec8",
   "metadata": {},
   "source": [
    "### build a model using the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374dc923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 10:34:25.914566: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-06 10:34:25.914709: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           270         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           930         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            39          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 10:34:30.385561: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-10-06 10:34:30.385700: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-06 10:34:30.385801: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-8E5U3B3): /proc/driver/nvidia/version does not exist\n",
      "2021-10-06 10:34:30.386825: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# set the seed \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# create all the layers of model\n",
    "\n",
    "## input layer\n",
    "input_layer = keras.layers.Input(shape=X_train_scaled.shape[1:])\n",
    "## hidden layers\n",
    "hidden_layer_1 = keras.layers.Dense(30, activation=\"relu\")(input_layer)\n",
    "hidden_layer_2 = keras.layers.Dense(30, activation=\"relu\")(hidden_layer_1)\n",
    "## concatenate layer\n",
    "concat = keras.layers.Concatenate()([input_layer, hidden_layer_2])\n",
    "## output layer\n",
    "output_layer = keras.layers.Dense(1)(concat)\n",
    "\n",
    "# create the model\n",
    "model = keras.models.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "# summarise\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a23e9",
   "metadata": {},
   "source": [
    "### compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2709727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 10:34:34.595054: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.2611 - val_loss: 3.3940\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.6580 - val_loss: 0.9360\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 897us/step - loss: 0.5878 - val_loss: 0.5649\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 900us/step - loss: 0.5582 - val_loss: 0.5712\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 928us/step - loss: 0.5347 - val_loss: 0.5045\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 938us/step - loss: 0.5158 - val_loss: 0.4831\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 929us/step - loss: 0.5002 - val_loss: 0.4639\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 942us/step - loss: 0.4876 - val_loss: 0.4638\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4760 - val_loss: 0.4421\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.4659 - val_loss: 0.4313\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 898us/step - loss: 0.4577 - val_loss: 0.4345\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 896us/step - loss: 0.4498 - val_loss: 0.4168\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.4428 - val_loss: 0.4230\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 921us/step - loss: 0.4366 - val_loss: 0.4047\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 944us/step - loss: 0.4307 - val_loss: 0.4078\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 948us/step - loss: 0.4257 - val_loss: 0.3938\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 939us/step - loss: 0.4210 - val_loss: 0.3952\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 979us/step - loss: 0.4167 - val_loss: 0.3860\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 955us/step - loss: 0.4121 - val_loss: 0.3827\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 903us/step - loss: 0.4088 - val_loss: 0.4054\n",
      "162/162 [==============================] - 0s 604us/step - loss: 0.4032\n"
     ]
    }
   ],
   "source": [
    "# compile the model with loss function and optimizer\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# create the history of the model validation\n",
    "history = model.fit(X_train_scaled, y_train, epochs=20,\n",
    "                    validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# evaluate the model with the test set\n",
    "mes_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103c573",
   "metadata": {},
   "source": [
    "### predict the results of certain new instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cbda0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[205.02869]\n",
      " [222.55455]\n",
      " [206.94267]]\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[: 3]\n",
    "\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb4824",
   "metadata": {},
   "source": [
    "### 2.4.2 what if we want to send a subset of the features through the wide path and a \n",
    "### different subset through the deep path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9f7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# set the seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# create the layers \n",
    "\n",
    "## input layers\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "\n",
    "## hidden layers\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "## concatenate layers\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "\n",
    "## output layer\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "\n",
    "# create the model\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73f021",
   "metadata": {},
   "source": [
    "### split the scaled sets into set As and set Bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5593b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_A, X_train_scaled_B = X_train_scaled[:, :5], X_train_scaled[:, 2:]\n",
    "X_valid_scaled_A, X_valid_scaled_B = X_valid_scaled[:, :5], X_valid_scaled[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07117c2e",
   "metadata": {},
   "source": [
    "### fit the model with the splited scaled sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e46014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8145 - val_loss: 0.8072\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6771 - val_loss: 0.6658\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5979 - val_loss: 0.5687\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5584 - val_loss: 0.5296\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.5334 - val_loss: 0.4993\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 959us/step - loss: 0.5120 - val_loss: 0.4811\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.4970 - val_loss: 0.4696\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4843 - val_loss: 0.4496\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4730 - val_loss: 0.4404\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.4644 - val_loss: 0.4315\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 979us/step - loss: 0.4570 - val_loss: 0.4268\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 973us/step - loss: 0.4510 - val_loss: 0.4166\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.4462 - val_loss: 0.4125\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.4421 - val_loss: 0.4074\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 954us/step - loss: 0.4385 - val_loss: 0.4044\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 948us/step - loss: 0.4356 - val_loss: 0.4007\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.4322 - val_loss: 0.4013\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 960us/step - loss: 0.4305 - val_loss: 0.3987\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.4274 - val_loss: 0.3934\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4204\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_scaled_A, X_train_scaled_B), y_train, epochs=20, \n",
    "                    validation_data=((X_valid_scaled_A, X_valid_scaled_B), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0914e",
   "metadata": {},
   "source": [
    "### evaluate the mse of data with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f645c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 989us/step - loss: 31885.6719\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e04d09",
   "metadata": {},
   "source": [
    "### predict the new instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f4c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151.92181]\n",
      " [167.28365]\n",
      " [153.67645]]\n"
     ]
    }
   ],
   "source": [
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1a7d6",
   "metadata": {},
   "source": [
    "### 2.4.3 multiple output models with functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f146eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "aux_output = keras.layers.Dense(1)(hidden2)\n",
    "\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1b3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the multi-output model\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a95bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7082 - dense_5_loss: 0.5954 - dense_6_loss: 1.7233 - val_loss: 1.6720 - val_dense_5_loss: 1.6486 - val_dense_6_loss: 1.8823\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5051 - dense_5_loss: 0.4519 - dense_6_loss: 0.9843 - val_loss: 7.4241 - val_dense_5_loss: 7.9438 - val_dense_6_loss: 2.7468\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5462 - dense_5_loss: 0.5123 - dense_6_loss: 0.8521 - val_loss: 0.4551 - val_dense_5_loss: 0.3918 - val_dense_6_loss: 1.0244\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4479 - dense_5_loss: 0.4154 - dense_6_loss: 0.7404 - val_loss: 0.5556 - val_dense_5_loss: 0.5335 - val_dense_6_loss: 0.7547\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4370 - dense_5_loss: 0.4112 - dense_6_loss: 0.6692 - val_loss: 0.4479 - val_dense_5_loss: 0.4065 - val_dense_6_loss: 0.8208\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4220 - dense_5_loss: 0.3988 - dense_6_loss: 0.6309 - val_loss: 0.5581 - val_dense_5_loss: 0.5502 - val_dense_6_loss: 0.6294\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4265 - dense_5_loss: 0.4068 - dense_6_loss: 0.6042 - val_loss: 0.3983 - val_dense_5_loss: 0.3689 - val_dense_6_loss: 0.6623\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4064 - dense_5_loss: 0.3868 - dense_6_loss: 0.5834 - val_loss: 0.3882 - val_dense_5_loss: 0.3682 - val_dense_6_loss: 0.5683\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3985 - dense_5_loss: 0.3796 - dense_6_loss: 0.5682 - val_loss: 0.3886 - val_dense_5_loss: 0.3710 - val_dense_6_loss: 0.5470\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3938 - dense_5_loss: 0.3754 - dense_6_loss: 0.5586 - val_loss: 0.3970 - val_dense_5_loss: 0.3819 - val_dense_6_loss: 0.5330\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3951 - dense_5_loss: 0.3779 - dense_6_loss: 0.5497 - val_loss: 0.6971 - val_dense_5_loss: 0.7026 - val_dense_6_loss: 0.6475\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3932 - dense_5_loss: 0.3765 - dense_6_loss: 0.5436 - val_loss: 0.7898 - val_dense_5_loss: 0.8102 - val_dense_6_loss: 0.6071\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3961 - dense_5_loss: 0.3803 - dense_6_loss: 0.5383 - val_loss: 0.6435 - val_dense_5_loss: 0.6312 - val_dense_6_loss: 0.7547\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3819 - dense_5_loss: 0.3655 - dense_6_loss: 0.5301 - val_loss: 0.3803 - val_dense_5_loss: 0.3670 - val_dense_6_loss: 0.5003\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3836 - dense_5_loss: 0.3685 - dense_6_loss: 0.5199 - val_loss: 0.4999 - val_dense_5_loss: 0.4904 - val_dense_6_loss: 0.5857\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3759 - dense_5_loss: 0.3604 - dense_6_loss: 0.5157 - val_loss: 0.3676 - val_dense_5_loss: 0.3527 - val_dense_6_loss: 0.5019\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3736 - dense_5_loss: 0.3584 - dense_6_loss: 0.5096 - val_loss: 0.3507 - val_dense_5_loss: 0.3356 - val_dense_6_loss: 0.4870\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3691 - dense_5_loss: 0.3542 - dense_6_loss: 0.5030 - val_loss: 0.3439 - val_dense_5_loss: 0.3288 - val_dense_6_loss: 0.4802\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3816 - dense_5_loss: 0.3683 - dense_6_loss: 0.5012 - val_loss: 0.3446 - val_dense_5_loss: 0.3304 - val_dense_6_loss: 0.4722\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - dense_5_loss: 0.3538 - dense_6_loss: 0.4924 - val_loss: 0.3702 - val_dense_5_loss: 0.3569 - val_dense_6_loss: 0.4902\n"
     ]
    }
   ],
   "source": [
    "# history\n",
    "history = model.fit([X_train_scaled_A, X_train_scaled_B], [y_train, y_train], epochs=20, \n",
    "                   validation_data=([X_valid_scaled_A, X_valid_scaled_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1387041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 965us/step - loss: 28947.0234 - dense_5_loss: 24904.5020 - dense_6_loss: 65329.7305\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3041a1",
   "metadata": {},
   "source": [
    "### 2.5 Saving and Restoring a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4bfec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"my_keras_model.h5\")\n",
    "\n",
    "# load the model\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df7cd9",
   "metadata": {},
   "source": [
    "### 2.6 Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d7aa8",
   "metadata": {},
   "source": [
    "### 2.6.1 establishing a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1177214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# clean the session before creating a new model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# create a new model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4306f0a0",
   "metadata": {},
   "source": [
    "### 2.6.2 compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b0e2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2556bf1",
   "metadata": {},
   "source": [
    "### 2.6.3 train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dc35a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8866 - val_loss: 0.7126\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 942us/step - loss: 0.6577 - val_loss: 0.6880\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5934 - val_loss: 0.5803\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 982us/step - loss: 0.5557 - val_loss: 0.5166\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 963us/step - loss: 0.5272 - val_loss: 0.4895\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 996us/step - loss: 0.5033 - val_loss: 0.4951\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.4861\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4709 - val_loss: 0.4554\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4413\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4474 - val_loss: 0.4379\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=10, \n",
    "                   validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb72b25",
   "metadata": {},
   "source": [
    "### 2.6.4 save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8262afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71037156",
   "metadata": {},
   "source": [
    "### 2.6.5 load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e3ac560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719ab9e",
   "metadata": {},
   "source": [
    "### 2.6.6 callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76abdd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.3695\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.3923 - val_loss: 0.3684\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3898 - val_loss: 0.3650\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 967us/step - loss: 0.3873 - val_loss: 0.3632\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 929us/step - loss: 0.3851 - val_loss: 0.3608\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 894us/step - loss: 0.3829 - val_loss: 0.3585\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.3564\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.3561\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.3552\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.3527\n"
     ]
    }
   ],
   "source": [
    "# obtain the checkpoints\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"keras_model.h5\", save_best_only=True)\n",
    "\n",
    "# fit the model with callback \n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, \n",
    "                   validation_data=(X_valid_scaled, y_valid), \n",
    "                   callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb0c326",
   "metadata": {},
   "source": [
    "### 2.6.7 save the callbacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05ceaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"keras_model_callbacked.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1890bf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f2edf971ee0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b2976",
   "metadata": {},
   "source": [
    "### 2.6.8 early stopping callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c65df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that is not with callback\n",
    "model = keras.models.load_model(\"keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da02537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3531\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.3715 - val_loss: 0.3568\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 977us/step - loss: 0.3700 - val_loss: 0.3501\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 976us/step - loss: 0.3685 - val_loss: 0.3478\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.3671 - val_loss: 0.3518\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.3658 - val_loss: 0.3532\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3488\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.3635 - val_loss: 0.3481\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 927us/step - loss: 0.3624 - val_loss: 0.3565\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.3612 - val_loss: 0.3550\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.3603 - val_loss: 0.3390\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 937us/step - loss: 0.3592 - val_loss: 0.3743\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 921us/step - loss: 0.3582 - val_loss: 0.3508\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 918us/step - loss: 0.3572 - val_loss: 0.3395\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.3562 - val_loss: 0.3636\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3347\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.3546\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.3539 - val_loss: 0.3430\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.3378\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3851\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.3448\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.3890\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3315\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3570\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3481\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3669\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3479 - val_loss: 0.3280\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3616\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 965us/step - loss: 0.3468 - val_loss: 0.3361\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 929us/step - loss: 0.3460 - val_loss: 0.3637\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 981us/step - loss: 0.3455 - val_loss: 0.3303\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3448 - val_loss: 0.3792\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3249\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.3439 - val_loss: 0.3325\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 817us/step - loss: 0.3434 - val_loss: 0.3530\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 816us/step - loss: 0.3431 - val_loss: 0.3254\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.3425 - val_loss: 0.3525\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.3420 - val_loss: 0.3254\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 806us/step - loss: 0.3415 - val_loss: 0.3357\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 895us/step - loss: 0.3410 - val_loss: 0.3271\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.3405 - val_loss: 0.3480\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3403 - val_loss: 0.3340\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 888us/step - loss: 0.3395 - val_loss: 0.3638\n"
     ]
    }
   ],
   "source": [
    "# callback with EarlyStopping\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, \n",
    "                                                 restore_best_weights=True)\n",
    "\n",
    "# history\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, \n",
    "                   validation_data=(X_valid_scaled, y_valid), \n",
    "                   callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7611b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3436\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "mse_test = model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5541c",
   "metadata": {},
   "source": [
    "### 2.7 Visualisation Using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77492226",
   "metadata": {},
   "source": [
    "### root log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b4314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the lib\n",
    "import os\n",
    "\n",
    "# define the root log directory\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34546b5",
   "metadata": {},
   "source": [
    "### subdirectory based on the current date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f684a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "import time\n",
    "\n",
    "# function\n",
    "def get_run_logdir():\n",
    "    \n",
    "    run_id = time.strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "    \n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# create the run log directory\n",
    "runlog_dir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f205dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_logs/run_2021_10_09_12_58_40'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runlog_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa2549",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b992bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# load the dataset\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d484da",
   "metadata": {},
   "source": [
    "### obtain scaled training, test and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb23150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into full training and test set\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "\n",
    "# split the full training set into training set and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# import the lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0692900",
   "metadata": {},
   "source": [
    "### create a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e418337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 13:08:34.883642: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-09 13:08:34.883872: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-09 13:08:34.887629: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-10-09 13:08:36.558823: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 76/363 [=====>........................] - ETA: 0s - loss: 4.5323"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 13:08:37.092792: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-09 13:08:37.092965: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-09 13:08:37.098477: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-10-09 13:08:37.106863: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-10-09 13:08:37.128705: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37\n",
      "\n",
      "2021-10-09 13:08:37.138374: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37/DESKTOP-8E5U3B3.trace.json.gz\n",
      "2021-10-09 13:08:37.157174: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37\n",
      "\n",
      "2021-10-09 13:08:37.158999: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37/DESKTOP-8E5U3B3.memory_profile.json.gz\n",
      "2021-10-09 13:08:37.167246: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37\n",
      "Dumped tool data for xplane.pb to ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37/DESKTOP-8E5U3B3.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37/DESKTOP-8E5U3B3.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37/DESKTOP-8E5U3B3.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37/DESKTOP-8E5U3B3.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./my_logs/run_2021_10_09_12_58_40/train/plugins/profile/2021_10_09_13_08_37/DESKTOP-8E5U3B3.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 2ms/step - loss: 1.8866 - val_loss: 0.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 13:08:37.870563: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6577 - val_loss: 0.6880\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 975us/step - loss: 0.5934 - val_loss: 0.5803\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5557 - val_loss: 0.5166\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.4895\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5033 - val_loss: 0.4951\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.4861\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4709 - val_loss: 0.4554\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4413\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4474 - val_loss: 0.4379\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4393 - val_loss: 0.4396\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4318 - val_loss: 0.4507\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.3997\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.3956\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.3916\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.4112 - val_loss: 0.3937\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.3809\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.4040 - val_loss: 0.3793\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.3850\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 971us/step - loss: 0.3980 - val_loss: 0.3809\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.3949 - val_loss: 0.3701\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.3924 - val_loss: 0.3781\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 0.3650\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.3655\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.3611\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 852us/step - loss: 0.3829 - val_loss: 0.3626\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.3809 - val_loss: 0.3564\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.3579\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3561\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3750 - val_loss: 0.3548\n",
      "INFO:tensorflow:Assets written to: ./my_logs/run_2021_10_09_12_58_40/assets\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# clear the session\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# create a model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# tensorboard callback\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(runlog_dir)\n",
    "\n",
    "# checkpoint callback\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(runlog_dir, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=30, \n",
    "                   validation_data=(X_valid_scaled, y_valid), \n",
    "                   callbacks=[checkpoint_cb, tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b4042",
   "metadata": {},
   "source": [
    "### start the TensorBoard Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00af8015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1995fa1660d0f36a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1995fa1660d0f36a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d43faa",
   "metadata": {},
   "source": [
    "## Section 3 Fine-Tuning Neural Network Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218cfc2",
   "metadata": {},
   "source": [
    "### load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a631c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# load the dataset\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5a965",
   "metadata": {},
   "source": [
    "### split the data into training, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03425ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into full training and test set\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "\n",
    "# split the full training set into training set and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# import the lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf006f",
   "metadata": {},
   "source": [
    "### clear the session and set new random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8fb83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 10:50:10.025770: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-11 10:50:10.026209: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import the lib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# clear the session\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# set new random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2c2a9",
   "metadata": {},
   "source": [
    "### define a function for buidling a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39d0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\": input_shape}\n",
    "    \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n",
    "        options = {}\n",
    "        \n",
    "    model.add(keras.layers.Dense(1, activation=\"relu\", **options))\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc0d3e",
   "metadata": {},
   "source": [
    "### create a KerasRegressor based on the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461b682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cbb4f",
   "metadata": {},
   "source": [
    "### fit with callback and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115963aa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 11:02:33.086642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-10-11 11:02:33.087193: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-11 11:02:33.087374: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-8E5U3B3): /proc/driver/nvidia/version does not exist\n",
      "2021-10-11 11:02:33.089495: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-11 11:02:34.172013: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0749 - val_loss: 0.5826\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.5664 - val_loss: 0.5011\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.5110 - val_loss: 0.4567\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 907us/step - loss: 0.4762 - val_loss: 0.4359\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 918us/step - loss: 0.4577 - val_loss: 0.4149\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 836us/step - loss: 0.4433 - val_loss: 0.4056\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.4352 - val_loss: 0.3971\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 835us/step - loss: 0.4281 - val_loss: 0.3928\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.4220 - val_loss: 0.3871\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.4172 - val_loss: 0.3832\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 943us/step - loss: 0.4131 - val_loss: 0.3801\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.4094 - val_loss: 0.3778\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 903us/step - loss: 0.4056 - val_loss: 0.3749\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.4025 - val_loss: 0.3730\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 842us/step - loss: 0.3996 - val_loss: 0.3704\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.3967 - val_loss: 0.3678\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 874us/step - loss: 0.3940 - val_loss: 0.3656\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 842us/step - loss: 0.3918 - val_loss: 0.3639\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 835us/step - loss: 0.3891 - val_loss: 0.3627\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 852us/step - loss: 0.3876 - val_loss: 0.3610\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 848us/step - loss: 0.3854 - val_loss: 0.3604\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3836 - val_loss: 0.3593\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 838us/step - loss: 0.3817 - val_loss: 0.3566\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3796 - val_loss: 0.3562\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3776 - val_loss: 0.3542\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 828us/step - loss: 0.3758 - val_loss: 0.3525\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 844us/step - loss: 0.3741 - val_loss: 0.3518\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3722 - val_loss: 0.3507\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.3709 - val_loss: 0.3489\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3688 - val_loss: 0.3485\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3671 - val_loss: 0.3472\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 836us/step - loss: 0.3655 - val_loss: 0.3464\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3646 - val_loss: 0.3444\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 875us/step - loss: 0.3628 - val_loss: 0.3438\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.3617 - val_loss: 0.3432\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3606 - val_loss: 0.3414\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 847us/step - loss: 0.3588 - val_loss: 0.3408\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.3570 - val_loss: 0.3987\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.3559 - val_loss: 0.3389\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 875us/step - loss: 0.3545 - val_loss: 0.4047\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 836us/step - loss: 0.3540 - val_loss: 0.3338\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3526 - val_loss: 0.4127\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.3523 - val_loss: 0.3342\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.3512 - val_loss: 0.4075\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.3512 - val_loss: 0.3333\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 886us/step - loss: 0.3498 - val_loss: 0.3485\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3493 - val_loss: 0.3307\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.3481 - val_loss: 0.3723\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.3490 - val_loss: 0.3321\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 819us/step - loss: 0.3476 - val_loss: 0.3372\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3467 - val_loss: 0.3286\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 917us/step - loss: 0.3461 - val_loss: 0.3310\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3462 - val_loss: 0.3308\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 828us/step - loss: 0.3449 - val_loss: 0.3302\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 835us/step - loss: 0.3447 - val_loss: 0.3439\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.3443 - val_loss: 0.3283\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.3436 - val_loss: 0.3285\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 902us/step - loss: 0.3426 - val_loss: 0.4213\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 822us/step - loss: 0.3440 - val_loss: 0.3269\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.3424 - val_loss: 0.3247\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3414 - val_loss: 0.3292\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 856us/step - loss: 0.3414 - val_loss: 0.4549\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3423 - val_loss: 0.3279\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.3422 - val_loss: 0.4356\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 855us/step - loss: 0.3410 - val_loss: 0.3255\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.3404 - val_loss: 0.3245\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.3389 - val_loss: 0.3244\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 841us/step - loss: 0.3388 - val_loss: 0.3236\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.3382 - val_loss: 0.3408\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 830us/step - loss: 0.3381 - val_loss: 0.3201\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 843us/step - loss: 0.3373 - val_loss: 0.3929\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 824us/step - loss: 0.3377 - val_loss: 0.3232\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 836us/step - loss: 0.3370 - val_loss: 0.4108\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 825us/step - loss: 0.3377 - val_loss: 0.3228\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 827us/step - loss: 0.3355 - val_loss: 0.6520\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 889us/step - loss: 0.3396 - val_loss: 0.3228\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.3364 - val_loss: 0.3231\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.3354 - val_loss: 0.3198\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.3342 - val_loss: 0.3669\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3347 - val_loss: 0.3199\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.3333 - val_loss: 0.3410\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 891us/step - loss: 0.3331 - val_loss: 0.3204\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 903us/step - loss: 0.3336 - val_loss: 0.6119\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.3366 - val_loss: 0.3207\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 934us/step - loss: 0.3333 - val_loss: 0.3201\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3323 - val_loss: 0.3191\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3316 - val_loss: 0.3176\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 942us/step - loss: 0.3305 - val_loss: 0.5062\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 869us/step - loss: 0.3330 - val_loss: 0.3175\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.3307 - val_loss: 0.3174\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 803us/step - loss: 0.3296 - val_loss: 0.4133\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.3303 - val_loss: 0.3165\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.3286 - val_loss: 0.5325\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 814us/step - loss: 0.3315 - val_loss: 0.3178\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 903us/step - loss: 0.3298 - val_loss: 0.3172\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 897us/step - loss: 0.3280 - val_loss: 0.3166\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.3274 - val_loss: 0.4868\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 900us/step - loss: 0.3287 - val_loss: 0.3169\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.3281 - val_loss: 0.3163\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 896us/step - loss: 0.3269 - val_loss: 0.3145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0ac2e0b50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train_scaled, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_valid_scaled, y_valid), \n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f9127",
   "metadata": {},
   "source": [
    "### obtain the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb153561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3299\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e13a7a",
   "metadata": {},
   "source": [
    "### Tune the hyperparameter with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab58f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7925 - val_loss: 1.6142\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 1.2029 - val_loss: 0.8172\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.7515 - val_loss: 0.6104\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.6306 - val_loss: 0.5509\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.5915 - val_loss: 0.5282\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.5730 - val_loss: 0.5144\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5600 - val_loss: 0.5034\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5488 - val_loss: 0.4935\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.5390 - val_loss: 0.4851\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5302 - val_loss: 0.4770\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5222 - val_loss: 0.4700\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5142 - val_loss: 0.4629\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5068 - val_loss: 0.4568\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5006 - val_loss: 0.4515\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4951 - val_loss: 0.4467\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4896 - val_loss: 0.4417\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4834 - val_loss: 0.4369\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4774 - val_loss: 0.4345\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4748 - val_loss: 0.4340\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4727 - val_loss: 0.4339\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4710 - val_loss: 0.4341\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4703 - val_loss: 0.4338\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4702 - val_loss: 0.4338\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4700 - val_loss: 0.4338\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4699 - val_loss: 0.4336\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4697 - val_loss: 0.4333\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4696 - val_loss: 0.4331\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.4693 - val_loss: 0.4329\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4693 - val_loss: 0.4327\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4693 - val_loss: 0.4327\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.4691 - val_loss: 0.4325\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.4691 - val_loss: 0.4327\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4690 - val_loss: 0.4324\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4688 - val_loss: 0.4321\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4690 - val_loss: 0.4321\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4689 - val_loss: 0.4321\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4689 - val_loss: 0.4322\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4687 - val_loss: 0.4320\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4686 - val_loss: 0.4317\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4688 - val_loss: 0.4319\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4687 - val_loss: 0.4322\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4685 - val_loss: 0.4320\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4687 - val_loss: 0.4319\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4687 - val_loss: 0.4319\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.4685 - val_loss: 0.4317\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4687 - val_loss: 0.4318\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4687 - val_loss: 0.4317\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.4687 - val_loss: 0.4319\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.4687 - val_loss: 0.4318\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4686 - val_loss: 0.4316\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4687 - val_loss: 0.4318\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4686 - val_loss: 0.4318\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.4686 - val_loss: 0.4318\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4687 - val_loss: 0.4319\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4686 - val_loss: 0.4319\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4686 - val_loss: 0.4319\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4686 - val_loss: 0.4321\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4685 - val_loss: 0.4318\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4686 - val_loss: 0.4318\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4686 - val_loss: 0.4320\n",
      "121/121 [==============================] - 0s 701us/step - loss: 0.4763\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3836 - val_loss: 2.2956\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 1.7759 - val_loss: 1.1852\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.9971 - val_loss: 0.7641\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.7333 - val_loss: 0.6278\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.6460 - val_loss: 0.5800\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.6110 - val_loss: 0.5561\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5908 - val_loss: 0.5398\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.5756 - val_loss: 0.5265\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5629 - val_loss: 0.5152\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5515 - val_loss: 0.5051\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5409 - val_loss: 0.4961\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5312 - val_loss: 0.4884\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5233 - val_loss: 0.4817\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5168 - val_loss: 0.4760\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5112 - val_loss: 0.4709\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5063 - val_loss: 0.4663\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5018 - val_loss: 0.4625\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.4983 - val_loss: 0.4589\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4947 - val_loss: 0.4558\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4919 - val_loss: 0.4532\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4894 - val_loss: 0.4508\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.4872 - val_loss: 0.4487\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.4851 - val_loss: 0.4470\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.4833 - val_loss: 0.4453\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4817 - val_loss: 0.4438\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4802 - val_loss: 0.4425\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4788 - val_loss: 0.4412\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4779 - val_loss: 0.4400\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.4765 - val_loss: 0.4394\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4759 - val_loss: 0.4385\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4750 - val_loss: 0.4374\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4743 - val_loss: 0.4368\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4734 - val_loss: 0.4363\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4730 - val_loss: 0.4357\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4724 - val_loss: 0.4353\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.4348\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4714 - val_loss: 0.4345\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.4709 - val_loss: 0.4342\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4705 - val_loss: 0.4337\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4703 - val_loss: 0.4336\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.4699 - val_loss: 0.4335\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4697 - val_loss: 0.4330\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4695 - val_loss: 0.4329\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4693 - val_loss: 0.4325\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4690 - val_loss: 0.4325\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4688 - val_loss: 0.4323\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4686 - val_loss: 0.4322\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4685 - val_loss: 0.4322\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4684 - val_loss: 0.4321\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4682 - val_loss: 0.4320\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4679 - val_loss: 0.4318\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4679 - val_loss: 0.4318\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.4678 - val_loss: 0.4316\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4676 - val_loss: 0.4317\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4676 - val_loss: 0.4317\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4675 - val_loss: 0.4315\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4676 - val_loss: 0.4314\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4674 - val_loss: 0.4314\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4673 - val_loss: 0.4314\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.4673 - val_loss: 0.4313\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4670 - val_loss: 0.4311\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4670 - val_loss: 0.4313\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4672 - val_loss: 0.4312\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4670 - val_loss: 0.4312\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4669 - val_loss: 0.4315\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4670 - val_loss: 0.4314\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4670 - val_loss: 0.4314\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4669 - val_loss: 0.4315\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4669 - val_loss: 0.4314\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4668 - val_loss: 0.4311\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.4667 - val_loss: 0.4312\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4668 - val_loss: 0.4313\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4668 - val_loss: 0.4311\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.4667 - val_loss: 0.4312\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4668 - val_loss: 0.4313\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4666 - val_loss: 0.4312\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4667 - val_loss: 0.4310\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4667 - val_loss: 0.4312\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.4666 - val_loss: 0.4313\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4666 - val_loss: 0.4312\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4666 - val_loss: 0.4313\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4668 - val_loss: 0.4312\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4665 - val_loss: 0.4314\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4667 - val_loss: 0.4314\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4665 - val_loss: 0.4313\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.4664 - val_loss: 0.4314\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4667 - val_loss: 0.4313\n",
      "121/121 [==============================] - 0s 555us/step - loss: 0.4798\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4548 - val_loss: 2.5018\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 1.8737 - val_loss: 1.1793\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.8957 - val_loss: 0.6117\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6019 - val_loss: 0.4984\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.5412 - val_loss: 0.4748\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5257 - val_loss: 0.4681\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.5197 - val_loss: 0.4645\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.5159 - val_loss: 0.4616\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5127 - val_loss: 0.4590\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.5102 - val_loss: 0.4565\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.5074 - val_loss: 0.4542\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.5051 - val_loss: 0.4523\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.5031 - val_loss: 0.4504\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5011 - val_loss: 0.4486\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4991 - val_loss: 0.4470\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.4975 - val_loss: 0.4456\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.4958 - val_loss: 0.4445\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.4945 - val_loss: 0.4434\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4931 - val_loss: 0.4421\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.4920 - val_loss: 0.4410\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.4909 - val_loss: 0.4402\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4899 - val_loss: 0.4394\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4889 - val_loss: 0.4389\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4883 - val_loss: 0.4397\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4880 - val_loss: 0.4384\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4864 - val_loss: 0.4382\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4851 - val_loss: 0.4365\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4849 - val_loss: 0.4359\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.4841 - val_loss: 0.4355\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4837 - val_loss: 0.4350\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4831 - val_loss: 0.4348\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4830 - val_loss: 0.4345\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4814 - val_loss: 0.4340\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.4817 - val_loss: 0.4339\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4817 - val_loss: 0.4339\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4807 - val_loss: 0.4337\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4802 - val_loss: 0.4340\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4796 - val_loss: 0.4339\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.4795 - val_loss: 0.4333\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4788 - val_loss: 0.4329\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4784 - val_loss: 0.4328\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.4784 - val_loss: 0.4330\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4770 - val_loss: 0.4324\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.4780 - val_loss: 0.4325\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4772 - val_loss: 0.4322\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.4772 - val_loss: 0.4322\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4771 - val_loss: 0.4321\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4764 - val_loss: 0.4320\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.4767 - val_loss: 0.4320\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.4766 - val_loss: 0.4321\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.4764 - val_loss: 0.4321\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.4762 - val_loss: 0.4320\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4759 - val_loss: 0.4320\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.4759 - val_loss: 0.4321\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4755 - val_loss: 0.4318\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.4752 - val_loss: 0.4317\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.4755 - val_loss: 0.4317\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4754 - val_loss: 0.4317\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4750 - val_loss: 0.4320\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4747 - val_loss: 0.4316\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.4747 - val_loss: 0.4315\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.4747 - val_loss: 0.4318\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4748 - val_loss: 0.4316\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4746 - val_loss: 0.4316\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.4748 - val_loss: 0.4317\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4747 - val_loss: 0.4316\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4746 - val_loss: 0.4316\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.4745 - val_loss: 0.4316\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.4746 - val_loss: 0.4317\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4746 - val_loss: 0.4315\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.4742 - val_loss: 0.4316\n",
      "121/121 [==============================] - 0s 575us/step - loss: 0.4635\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7468 - val_loss: 0.5547\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5745 - val_loss: 0.4981\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.5312 - val_loss: 0.4618\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4991 - val_loss: 0.4420\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.4817 - val_loss: 0.4420\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.4740 - val_loss: 0.4344\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4709 - val_loss: 0.4339\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4709 - val_loss: 0.4337\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.4706 - val_loss: 0.4339\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4709 - val_loss: 0.4308\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.4717 - val_loss: 0.4385\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4719 - val_loss: 0.4354\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4696 - val_loss: 0.4320\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.4718 - val_loss: 0.4334\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4697 - val_loss: 0.4313\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.4711 - val_loss: 0.4316\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4707 - val_loss: 0.4364\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.4707 - val_loss: 0.4322\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.4698 - val_loss: 0.4329\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4717 - val_loss: 0.4316\n",
      "121/121 [==============================] - 0s 606us/step - loss: 0.4723\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3123 - val_loss: 0.6729\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.6551 - val_loss: 0.5582\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5746 - val_loss: 0.5007\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.5276 - val_loss: 0.4673\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4954 - val_loss: 0.4479\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4813 - val_loss: 0.4398\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.4758 - val_loss: 0.4373\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4722 - val_loss: 0.4338\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.4696 - val_loss: 0.4331\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4695 - val_loss: 0.4313\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.4694 - val_loss: 0.4323\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.4679 - val_loss: 0.4331\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.4684 - val_loss: 0.4322\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.4702 - val_loss: 0.4331\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.4681 - val_loss: 0.4317\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.4689 - val_loss: 0.4314\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.4669 - val_loss: 0.4333\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.4699 - val_loss: 0.4325\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.4673 - val_loss: 0.4321\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.4677 - val_loss: 0.4334\n",
      "121/121 [==============================] - 0s 619us/step - loss: 0.4867\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9449 - val_loss: 0.6703\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.6106 - val_loss: 0.5189\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5479 - val_loss: 0.4848\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5197 - val_loss: 0.4602\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5043 - val_loss: 0.4485\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.4990 - val_loss: 0.4453\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4928 - val_loss: 0.4380\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4965 - val_loss: 0.4343\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.4818 - val_loss: 0.4340\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.4777 - val_loss: 0.4321\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4799 - val_loss: 0.4337\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.4795 - val_loss: 0.4381\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.4749 - val_loss: 0.4333\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4800 - val_loss: 0.4323\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4797 - val_loss: 0.4321\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4761 - val_loss: 0.4318\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.4768 - val_loss: 0.4335\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.4781 - val_loss: 0.4355\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.4776 - val_loss: 0.4316\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.4780 - val_loss: 0.4326\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4793 - val_loss: 0.4324\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4783 - val_loss: 0.4329\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.4801 - val_loss: 0.4340\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4744 - val_loss: 0.4337\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4775 - val_loss: 0.4327\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4738 - val_loss: 0.4348\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4763 - val_loss: 0.4331\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4774 - val_loss: 0.4444\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4822 - val_loss: 0.4336\n",
      "121/121 [==============================] - 0s 740us/step - loss: 0.4649\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.8481 - val_loss: 3.9811\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2554 - val_loss: 1.0069\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8490 - val_loss: 0.7550\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7553 - val_loss: 0.6895\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7076 - val_loss: 0.6507\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6751 - val_loss: 0.6219\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6492 - val_loss: 0.5982\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6272 - val_loss: 0.5784\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6078 - val_loss: 0.5609\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5902 - val_loss: 0.5446\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5742 - val_loss: 0.5301\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5595 - val_loss: 0.5164\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5461 - val_loss: 0.5038\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5334 - val_loss: 0.4928\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5218 - val_loss: 0.4814\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5109 - val_loss: 0.4719\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5010 - val_loss: 0.4630\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4919 - val_loss: 0.4548\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4833 - val_loss: 0.4479\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4758 - val_loss: 0.4405\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4687 - val_loss: 0.4345\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4622 - val_loss: 0.4288\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4562 - val_loss: 0.4235\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4506 - val_loss: 0.4191\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.4146\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4412 - val_loss: 0.4107\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.4071\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4039\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.4011\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4262 - val_loss: 0.3983\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.3957\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.3937\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4174 - val_loss: 0.3914\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.3892\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.3876\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4100 - val_loss: 0.3860\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.3839\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4059 - val_loss: 0.3827\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.3808\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.3798\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4001 - val_loss: 0.3788\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3984 - val_loss: 0.3774\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3970 - val_loss: 0.3762\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.3755\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.3740\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.3732\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3910 - val_loss: 0.3720\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 0.3711\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3881 - val_loss: 0.3700\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3869 - val_loss: 0.3691\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.3685\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.3672\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3832 - val_loss: 0.3664\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3820 - val_loss: 0.3656\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.3648\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.3640\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.3632\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3623\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.3613\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.3610\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3599\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.3596\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.3583\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3574\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3694 - val_loss: 0.3567\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.3560\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.3600\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.3705\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.3699\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.3654\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3630 - val_loss: 0.3789\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3694\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.3595\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.3743\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3753\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.3738\n",
      "121/121 [==============================] - 0s 574us/step - loss: 0.3800\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 2.7809 - val_loss: 3.7884\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9574 - val_loss: 5.0906\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7972 - val_loss: 3.8328\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7335 - val_loss: 2.7007\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 1.9608\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6620 - val_loss: 1.4461\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6358 - val_loss: 1.0643\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.7941\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5919 - val_loss: 0.6478\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5733 - val_loss: 0.5615\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5564 - val_loss: 0.5283\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5411 - val_loss: 0.5142\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5276 - val_loss: 0.5007\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5152 - val_loss: 0.4900\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5042 - val_loss: 0.4781\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4942 - val_loss: 0.4694\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4852 - val_loss: 0.4601\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4771 - val_loss: 0.4524\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4696 - val_loss: 0.4454\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4630 - val_loss: 0.4394\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4569 - val_loss: 0.4330\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.4283\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4463 - val_loss: 0.4234\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4417 - val_loss: 0.4189\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4149\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4334 - val_loss: 0.4111\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4296 - val_loss: 0.4083\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4262 - val_loss: 0.4048\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.4037\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.3997\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.3966\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4145 - val_loss: 0.3946\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.3926\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.3913\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4075 - val_loss: 0.3893\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.3867\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.3855\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.3838\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3996 - val_loss: 0.3817\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3980 - val_loss: 0.3808\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.3798\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.3788\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3930 - val_loss: 0.3766\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3916 - val_loss: 0.3756\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.3752\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.3728\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.3717\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.3706\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.3701\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.3688\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3676\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.3665\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.3655\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.3647\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3783 - val_loss: 0.3641\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.3630\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.3618\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.3615\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3608\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3598\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3592\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.3589\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3706 - val_loss: 0.3573\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.3570\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3687 - val_loss: 0.3573\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3681 - val_loss: 0.3568\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.3551\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3664 - val_loss: 0.3552\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.3532\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3649 - val_loss: 0.3511\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3642 - val_loss: 0.3495\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.3496\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.3517\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.3597\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.3632\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.3752\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3770\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.3906\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.3997\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3585 - val_loss: 0.4150\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.4288\n",
      "121/121 [==============================] - 0s 715us/step - loss: 0.3727\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.3151 - val_loss: 1.7071\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1237 - val_loss: 1.0042\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7809 - val_loss: 0.7050\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6887 - val_loss: 0.6364\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6459 - val_loss: 0.6013\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6178 - val_loss: 0.5769\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5952 - val_loss: 0.5577\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5765 - val_loss: 0.5388\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5602 - val_loss: 0.5235\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5451 - val_loss: 0.5086\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5313 - val_loss: 0.4960\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.4836\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5070 - val_loss: 0.4727\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4967 - val_loss: 0.4638\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4870 - val_loss: 0.4546\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4783 - val_loss: 0.4457\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4703 - val_loss: 0.4386\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4628 - val_loss: 0.4317\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4562 - val_loss: 0.4255\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4499 - val_loss: 0.4198\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.4145\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4101\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4342 - val_loss: 0.4059\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4026\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.3981\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.3948\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.3913\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4144 - val_loss: 0.3886\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.3866\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.3811\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.3783\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4018 - val_loss: 0.3807\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3990 - val_loss: 0.3750\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3967 - val_loss: 0.3846\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3946 - val_loss: 0.3723\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.3705\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.3700\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3882 - val_loss: 0.3731\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.3672\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.3660\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3824 - val_loss: 0.3810\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3811 - val_loss: 0.3791\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.3615\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3780 - val_loss: 0.3727\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3765 - val_loss: 0.3671\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3668\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3737 - val_loss: 0.3582\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3589\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.3776\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.3762\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.3721\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3678 - val_loss: 0.3754\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.3618\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.3585\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3647 - val_loss: 0.3606\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.3695\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.3750\n",
      "121/121 [==============================] - 0s 655us/step - loss: 0.3611\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.8244 - val_loss: 5.2490\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0216 - val_loss: 3.7017\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5154 - val_loss: 2.3485\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2452 - val_loss: 1.5052\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0639 - val_loss: 1.0676\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9319 - val_loss: 0.8825\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.8340 - val_loss: 0.7843\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7626 - val_loss: 0.7137\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7105 - val_loss: 0.6629\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6730 - val_loss: 0.6263\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6456 - val_loss: 0.6006\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6252 - val_loss: 0.5808\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6090 - val_loss: 0.5654\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5957 - val_loss: 0.5540\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5847 - val_loss: 0.5424\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5746 - val_loss: 0.5334\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5661 - val_loss: 0.5252\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5581 - val_loss: 0.5177\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.5117\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5440 - val_loss: 0.5046\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5377 - val_loss: 0.4990\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.4934\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.5261 - val_loss: 0.4883\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.4835\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5159 - val_loss: 0.4788\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5110 - val_loss: 0.4745\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5066 - val_loss: 0.4704\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5023 - val_loss: 0.4665\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4982 - val_loss: 0.4630\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4944 - val_loss: 0.4594\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4907 - val_loss: 0.4560\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4872 - val_loss: 0.4530\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4838 - val_loss: 0.4500\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4805 - val_loss: 0.4469\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4774 - val_loss: 0.4443\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4745 - val_loss: 0.4417\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4717 - val_loss: 0.4390\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4690 - val_loss: 0.4367\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4662 - val_loss: 0.4343\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.4323\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4612 - val_loss: 0.4303\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4588 - val_loss: 0.4282\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4566 - val_loss: 0.4260\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4541 - val_loss: 0.4245\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.4223\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.4205\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4481 - val_loss: 0.4186\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4459 - val_loss: 0.4168\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4440 - val_loss: 0.4152\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4135\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4123\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4387 - val_loss: 0.4104\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4368 - val_loss: 0.4089\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4350 - val_loss: 0.4074\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4060\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4317 - val_loss: 0.4047\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4301 - val_loss: 0.4034\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4020\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4270 - val_loss: 0.4006\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.3995\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.3980\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.3969\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4211 - val_loss: 0.3957\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.3942\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.3930\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.3918\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.3906\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4144 - val_loss: 0.3897\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.3885\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.3877\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.3864\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.3854\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4081 - val_loss: 0.3844\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4070 - val_loss: 0.3834\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.3824\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.3818\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.3806\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.3797\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.3788\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.3781\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3994 - val_loss: 0.3770\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.3764\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3973 - val_loss: 0.3755\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.3749\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.3739\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.3734\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.3722\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.3717\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3916 - val_loss: 0.3708\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.3701\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.3693\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.3686\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3882 - val_loss: 0.3679\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.3674\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.3668\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.3667\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.3654\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.3649\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3830 - val_loss: 0.3653\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3826 - val_loss: 0.3637\n",
      "121/121 [==============================] - 0s 591us/step - loss: 0.3972\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.7741 - val_loss: 3.0404\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7861 - val_loss: 3.0979\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2034 - val_loss: 2.7036\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9662 - val_loss: 2.1190\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8198 - val_loss: 1.6006\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7269 - val_loss: 1.2975\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.6680 - val_loss: 1.0791\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6297 - val_loss: 0.9133\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6027 - val_loss: 0.7939\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5822 - val_loss: 0.7066\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5652 - val_loss: 0.6426\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.5887\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5376 - val_loss: 0.5509\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5258 - val_loss: 0.5239\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5151 - val_loss: 0.5000\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5053 - val_loss: 0.4852\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4964 - val_loss: 0.4726\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4882 - val_loss: 0.4637\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4808 - val_loss: 0.4574\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4743 - val_loss: 0.4515\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4684 - val_loss: 0.4451\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4632 - val_loss: 0.4399\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4354\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 0.4304\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4507 - val_loss: 0.4263\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4228\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4444 - val_loss: 0.4202\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4418 - val_loss: 0.4169\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4160\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4373 - val_loss: 0.4122\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4351 - val_loss: 0.4098\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4332 - val_loss: 0.4081\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4064\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4053\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.4037\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4267 - val_loss: 0.4014\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.4007\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4239 - val_loss: 0.3991\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.3980\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.3969\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.3959\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.3951\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.3934\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4169 - val_loss: 0.3928\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4158 - val_loss: 0.3919\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.3904\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4138 - val_loss: 0.3894\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.3883\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.3874\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.3859\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.3848\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.3837\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.4083 - val_loss: 0.3826\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4075 - val_loss: 0.3818\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.3812\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4058 - val_loss: 0.3806\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.3796\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.3798\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.3794\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.3789\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4020 - val_loss: 0.3797\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.3802\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.3796\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3999 - val_loss: 0.3806\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3990 - val_loss: 0.3825\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3984 - val_loss: 0.3834\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3979 - val_loss: 0.3826\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.3851\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.3862\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3958 - val_loss: 0.3852\n",
      "121/121 [==============================] - 0s 623us/step - loss: 0.4024\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.2646 - val_loss: 3.8575\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2687 - val_loss: 3.2089\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8337 - val_loss: 2.0116\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7158 - val_loss: 1.5079\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6680 - val_loss: 1.1711\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6397 - val_loss: 0.9634\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6187 - val_loss: 0.8423\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6015 - val_loss: 0.7509\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.7057\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5731 - val_loss: 0.6704\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5610 - val_loss: 0.6491\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5499 - val_loss: 0.6189\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5400 - val_loss: 0.6018\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.5888\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.5774\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5661\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5070 - val_loss: 0.5517\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5002 - val_loss: 0.5422\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4942 - val_loss: 0.5365\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4883 - val_loss: 0.5284\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4829 - val_loss: 0.5211\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4778 - val_loss: 0.5157\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4732 - val_loss: 0.5080\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4686 - val_loss: 0.5074\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4646 - val_loss: 0.5018\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4607 - val_loss: 0.4957\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4571 - val_loss: 0.4930\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4536 - val_loss: 0.4867\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4503 - val_loss: 0.4885\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4818\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4445 - val_loss: 0.4762\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4417 - val_loss: 0.4786\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4393 - val_loss: 0.4714\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4367 - val_loss: 0.4732\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4345 - val_loss: 0.4706\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4642\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4303 - val_loss: 0.4612\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 0.4594\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4263 - val_loss: 0.4591\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4245 - val_loss: 0.4551\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.4595\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4212 - val_loss: 0.4573\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.4520\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4182 - val_loss: 0.4543\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4168 - val_loss: 0.4539\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4495\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.4496\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.4499\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.4502\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4498\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.4462\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4088 - val_loss: 0.4454\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.4455\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4428\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.4444\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.4441\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.4435\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.4422\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.4417\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 0.4390\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.4422\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.4401\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3997 - val_loss: 0.4381\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.4373\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4378\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3975 - val_loss: 0.4388\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3970 - val_loss: 0.4347\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.4343\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.4334\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.4323\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.4303\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.4295\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3931 - val_loss: 0.4273\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3927 - val_loss: 0.4277\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.4284\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3915 - val_loss: 0.4276\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.4246\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.4225\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.4233\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.4277\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3889 - val_loss: 0.4255\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3884 - val_loss: 0.4223\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3879 - val_loss: 0.4223\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.4221\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.4186\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3864 - val_loss: 0.4197\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3858 - val_loss: 0.4202\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3853 - val_loss: 0.4198\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.4195\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3844 - val_loss: 0.4169\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.4137\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3835 - val_loss: 0.4165\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3830 - val_loss: 0.4140\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.4149\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3820 - val_loss: 0.4143\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.4118\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.4139\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4126\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3803 - val_loss: 0.4100\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.4101\n",
      "121/121 [==============================] - 0s 652us/step - loss: 0.3825\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6191 - val_loss: 0.7843\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.6812 - val_loss: 0.5704\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.6042 - val_loss: 0.5265\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5646 - val_loss: 0.4957\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.5334 - val_loss: 0.4691\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.5093 - val_loss: 0.4519\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4887 - val_loss: 0.4414\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4800 - val_loss: 0.4414\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4769 - val_loss: 0.4381\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.4742 - val_loss: 0.4335\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4740 - val_loss: 0.4387\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4733 - val_loss: 0.4356\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4704 - val_loss: 0.4323\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.4722 - val_loss: 0.4336\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4700 - val_loss: 0.4314\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4712 - val_loss: 0.4317\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4704 - val_loss: 0.4353\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.4704 - val_loss: 0.4319\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4696 - val_loss: 0.4321\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4710 - val_loss: 0.4315\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.4711 - val_loss: 0.4338\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4688 - val_loss: 0.4321\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.4703 - val_loss: 0.4386\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4707 - val_loss: 0.4355\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4702 - val_loss: 0.4339\n",
      "121/121 [==============================] - 0s 532us/step - loss: 0.4822\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7906 - val_loss: 1.4198\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.6830 - val_loss: 0.5098\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5330 - val_loss: 0.4770\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5055 - val_loss: 0.4591\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.4894 - val_loss: 0.4480\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4806 - val_loss: 0.4411\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4756 - val_loss: 0.4383\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4720 - val_loss: 0.4352\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4697 - val_loss: 0.4339\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4691 - val_loss: 0.4325\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4687 - val_loss: 0.4326\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4677 - val_loss: 0.4329\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4679 - val_loss: 0.4321\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4688 - val_loss: 0.4327\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4678 - val_loss: 0.4316\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4679 - val_loss: 0.4314\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4665 - val_loss: 0.4323\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4687 - val_loss: 0.4314\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4667 - val_loss: 0.4320\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4675 - val_loss: 0.4324\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4681 - val_loss: 0.4321\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4683 - val_loss: 0.4320\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.4677 - val_loss: 0.4325\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4677 - val_loss: 0.4321\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4680 - val_loss: 0.4319\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4675 - val_loss: 0.4317\n",
      "121/121 [==============================] - 0s 590us/step - loss: 0.4782\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9580 - val_loss: 2.8007\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 1.4163 - val_loss: 0.5563\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.5570 - val_loss: 0.4831\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5238 - val_loss: 0.4641\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.5047 - val_loss: 0.4492\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4915 - val_loss: 0.4443\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4859 - val_loss: 0.4396\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4861 - val_loss: 0.4363\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.4825 - val_loss: 0.4359\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.4769 - val_loss: 0.4336\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4818 - val_loss: 0.4338\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.4780 - val_loss: 0.4360\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.4742 - val_loss: 0.4334\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4791 - val_loss: 0.4331\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.4780 - val_loss: 0.4324\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.4754 - val_loss: 0.4320\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4765 - val_loss: 0.4326\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4764 - val_loss: 0.4342\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4786 - val_loss: 0.4348\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4756 - val_loss: 0.4322\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4772 - val_loss: 0.4323\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4768 - val_loss: 0.4324\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4779 - val_loss: 0.4331\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4741 - val_loss: 0.4331\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.4764 - val_loss: 0.4325\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4738 - val_loss: 0.4337\n",
      "121/121 [==============================] - 0s 583us/step - loss: 0.4655\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0758 - val_loss: 0.5398\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.5447 - val_loss: 0.4566\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4780 - val_loss: 0.4173\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4542 - val_loss: 0.3988\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4383 - val_loss: 0.3979\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.4221 - val_loss: 0.3860\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4119 - val_loss: 0.3778\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.4049 - val_loss: 0.3741\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4005 - val_loss: 0.3754\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3968 - val_loss: 0.3645\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3933 - val_loss: 0.3667\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3897 - val_loss: 0.3693\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3862 - val_loss: 0.3605\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3850 - val_loss: 0.3583\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3812 - val_loss: 0.3559\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3800 - val_loss: 0.3552\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.3526\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3772 - val_loss: 0.3523\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3728 - val_loss: 0.3524\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3709 - val_loss: 0.3492\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3510\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3684 - val_loss: 0.3493\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3659 - val_loss: 0.3571\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3667 - val_loss: 0.3494\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.3621 - val_loss: 0.3528\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3617 - val_loss: 0.3477\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3623 - val_loss: 0.3428\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3689 - val_loss: 0.3459\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3610 - val_loss: 0.3401\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.3388\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3359\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3346\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3498 - val_loss: 0.3347\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3485 - val_loss: 0.3318\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3465 - val_loss: 0.3331\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.3448 - val_loss: 0.3325\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3467 - val_loss: 0.3535\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.3532 - val_loss: 0.3356\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3471 - val_loss: 0.3351\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3450 - val_loss: 0.3301\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3450 - val_loss: 0.3360\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3419 - val_loss: 0.3285\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3421 - val_loss: 0.3282\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3391 - val_loss: 0.3289\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3408 - val_loss: 0.3273\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.3282\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3357 - val_loss: 0.3248\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3368 - val_loss: 0.3270\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3317 - val_loss: 0.3222\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3313 - val_loss: 0.4973\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3351 - val_loss: 0.3456\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3329 - val_loss: 0.3235\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3318 - val_loss: 0.3211\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3301 - val_loss: 0.3238\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3286 - val_loss: 0.3260\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3297 - val_loss: 0.3179\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3249 - val_loss: 2.3855\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3798 - val_loss: 0.3378\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3439 - val_loss: 0.3269\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3364 - val_loss: 0.3283\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3341 - val_loss: 0.3241\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3304 - val_loss: 0.3277\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3295 - val_loss: 0.3226\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3273 - val_loss: 0.3210\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3261 - val_loss: 0.3186\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3318 - val_loss: 0.3223\n",
      "121/121 [==============================] - 0s 661us/step - loss: 0.3698\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9709 - val_loss: 0.5077\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.4874 - val_loss: 0.4192\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4334 - val_loss: 0.3954\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4166 - val_loss: 0.3806\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4088\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3944 - val_loss: 0.7677\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 1.1702\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3835 - val_loss: 1.4085\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.3787 - val_loss: 1.5926\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3774 - val_loss: 1.0675\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3713 - val_loss: 1.5222\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3681 - val_loss: 1.5174\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3670 - val_loss: 1.7652\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3673 - val_loss: 1.4070\n",
      "121/121 [==============================] - 0s 597us/step - loss: 0.3903\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0274 - val_loss: 0.5253\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.4972 - val_loss: 0.4251\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4387 - val_loss: 0.3944\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4219 - val_loss: 0.3833\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4195 - val_loss: 0.3807\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.4119 - val_loss: 0.3752\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4081 - val_loss: 0.3670\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3942 - val_loss: 0.3661\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3908 - val_loss: 0.3613\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3891 - val_loss: 0.3578\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3858 - val_loss: 0.3937\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3857 - val_loss: 0.3709\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3842 - val_loss: 0.3551\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3750 - val_loss: 0.3546\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3720 - val_loss: 1.0728\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3827 - val_loss: 0.3479\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3750 - val_loss: 0.3500\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3762 - val_loss: 0.3534\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3679 - val_loss: 0.3431\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3626 - val_loss: 0.3402\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3818 - val_loss: 0.5847\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3913 - val_loss: 0.3472\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3743 - val_loss: 0.3567\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3678 - val_loss: 0.3453\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3638 - val_loss: 0.3381\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3616 - val_loss: 0.3405\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3616 - val_loss: 0.3354\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3784\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4797 - val_loss: 0.3814\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3936 - val_loss: 3.3539\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.3986 - val_loss: 0.3566\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3831 - val_loss: 0.3442\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.3453\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3683 - val_loss: 0.3462\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3638 - val_loss: 0.3394\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.4859\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.3498\n",
      "121/121 [==============================] - 0s 713us/step - loss: 0.3622\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 5.5724 - val_loss: 5.3834\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1233 - val_loss: 4.1543\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1211 - val_loss: 2.3464\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5105 - val_loss: 2.7277\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0274 - val_loss: 2.7145\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8974 - val_loss: 2.4210\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.8330 - val_loss: 2.0871\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.7877 - val_loss: 1.7981\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7534 - val_loss: 1.5596\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7265 - val_loss: 1.3792\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.7050 - val_loss: 1.2333\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.6874 - val_loss: 1.0875\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6723 - val_loss: 0.9844\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6591 - val_loss: 0.9099\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6473 - val_loss: 0.8309\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6364 - val_loss: 0.7704\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6265 - val_loss: 0.7328\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6173 - val_loss: 0.6974\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6086 - val_loss: 0.6687\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.6007 - val_loss: 0.6352\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.5931 - val_loss: 0.6120\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5857 - val_loss: 0.5906\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5789 - val_loss: 0.5747\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5722 - val_loss: 0.5563\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5660 - val_loss: 0.5448\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5597 - val_loss: 0.5329\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5539 - val_loss: 0.5240\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5481 - val_loss: 0.5154\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5426 - val_loss: 0.5084\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5373 - val_loss: 0.5017\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.4956\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5270 - val_loss: 0.4907\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.4859\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5173 - val_loss: 0.4814\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5126 - val_loss: 0.4773\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5082 - val_loss: 0.4730\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5040 - val_loss: 0.4694\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4999 - val_loss: 0.4656\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4959 - val_loss: 0.4624\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4921 - val_loss: 0.4594\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4883 - val_loss: 0.4565\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.4847 - val_loss: 0.4534\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4812 - val_loss: 0.4505\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4778 - val_loss: 0.4484\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4746 - val_loss: 0.4458\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4714 - val_loss: 0.4429\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4686 - val_loss: 0.4411\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4656 - val_loss: 0.4391\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4375\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4604 - val_loss: 0.4353\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4329\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4555 - val_loss: 0.4317\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4531 - val_loss: 0.4309\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.4297\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4486 - val_loss: 0.4288\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4466 - val_loss: 0.4279\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4446 - val_loss: 0.4268\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4427 - val_loss: 0.4267\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4409 - val_loss: 0.4256\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4389 - val_loss: 0.4243\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4246\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4244\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4339 - val_loss: 0.4230\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4324 - val_loss: 0.4234\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4307 - val_loss: 0.4236\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.4229\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4276 - val_loss: 0.4218\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4262 - val_loss: 0.4216\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4248 - val_loss: 0.4204\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4233 - val_loss: 0.4196\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.4197\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4206 - val_loss: 0.4199\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4193 - val_loss: 0.4190\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4184\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4168 - val_loss: 0.4180\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.4154 - val_loss: 0.4171\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4143 - val_loss: 0.4171\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 0.4176\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.4169\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.4147\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.4158\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4084 - val_loss: 0.4178\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4074 - val_loss: 0.4161\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.4165\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4052 - val_loss: 0.4129\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4039 - val_loss: 0.4164\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4033 - val_loss: 0.4140\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.4129\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4013 - val_loss: 0.4129\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.4135\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3994 - val_loss: 0.4121\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3984 - val_loss: 0.4112\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3975 - val_loss: 0.4100\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.4100\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3958 - val_loss: 0.4103\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.4114\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3941 - val_loss: 0.4092\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3932 - val_loss: 0.4068\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.4110\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3917 - val_loss: 0.4074\n",
      "121/121 [==============================] - 0s 662us/step - loss: 0.4070\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4207 - val_loss: 5.4761\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4753 - val_loss: 9.1962\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5623 - val_loss: 12.9832\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2528 - val_loss: 14.5135\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1058 - val_loss: 14.5151\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0086 - val_loss: 14.0736\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9422 - val_loss: 13.5833\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8961 - val_loss: 12.7494\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8620 - val_loss: 11.9751\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8353 - val_loss: 11.2486\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8133 - val_loss: 10.5811\n",
      "121/121 [==============================] - 0s 675us/step - loss: 1.0423\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 4.7577 - val_loss: 3.4166\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5230 - val_loss: 2.2556\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5416 - val_loss: 1.7134\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1871 - val_loss: 1.3089\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0017 - val_loss: 1.0324\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8839 - val_loss: 0.8723\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8051 - val_loss: 0.7737\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7520 - val_loss: 0.7086\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7147 - val_loss: 0.6699\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6875 - val_loss: 0.6411\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6669 - val_loss: 0.6209\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6504 - val_loss: 0.6046\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6365 - val_loss: 0.5911\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6244 - val_loss: 0.5805\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6136 - val_loss: 0.5702\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6039 - val_loss: 0.5601\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5949 - val_loss: 0.5523\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5865 - val_loss: 0.5449\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5789 - val_loss: 0.5372\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5717 - val_loss: 0.5302\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5649 - val_loss: 0.5233\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5586 - val_loss: 0.5178\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5526 - val_loss: 0.5126\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5467 - val_loss: 0.5093\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5413 - val_loss: 0.5033\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5360 - val_loss: 0.4984\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 0.4922\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5261 - val_loss: 0.4888\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.4861\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5171 - val_loss: 0.4810\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5127 - val_loss: 0.4754\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5086 - val_loss: 0.4724\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5045 - val_loss: 0.4672\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5008 - val_loss: 0.4670\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4972 - val_loss: 0.4635\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4935 - val_loss: 0.4588\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4901 - val_loss: 0.4572\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4867 - val_loss: 0.4543\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4834 - val_loss: 0.4498\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4802 - val_loss: 0.4465\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4770 - val_loss: 0.4483\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4743 - val_loss: 0.4450\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4713 - val_loss: 0.4392\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4686 - val_loss: 0.4391\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4659 - val_loss: 0.4353\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4633 - val_loss: 0.4332\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4608 - val_loss: 0.4303\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4583 - val_loss: 0.4283\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4560 - val_loss: 0.4291\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4537 - val_loss: 0.4281\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4514 - val_loss: 0.4267\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4493 - val_loss: 0.4256\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4472 - val_loss: 0.4242\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4452 - val_loss: 0.4219\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4430 - val_loss: 0.4189\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4410 - val_loss: 0.4158\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4145\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4165\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.4163\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4102\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4321 - val_loss: 0.4094\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4303 - val_loss: 0.4147\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4083\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.4046\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.4045\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4243 - val_loss: 0.4049\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.3996\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4216 - val_loss: 0.4001\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.4017\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4000\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.3965\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4166 - val_loss: 0.4011\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.3965\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4143 - val_loss: 0.3966\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4132 - val_loss: 0.3981\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.3957\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4112 - val_loss: 0.3947\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.3940\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4092 - val_loss: 0.3931\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4082 - val_loss: 0.3997\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4074 - val_loss: 0.3983\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3928\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.3914\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.3935\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4039 - val_loss: 0.3892\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.3961\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.3939\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 0.3938\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.3969\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.3912\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.3907\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3988 - val_loss: 0.3885\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.3909\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3975 - val_loss: 0.3907\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.3901\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.3915\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.3973\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.3889\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3944 - val_loss: 0.3942\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.3936\n",
      "121/121 [==============================] - 0s 705us/step - loss: 0.3880\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5787 - val_loss: 0.6012\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5994 - val_loss: 0.5088\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.5215 - val_loss: 0.4562\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4758 - val_loss: 0.4298\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4583 - val_loss: 0.4169\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4074\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4316 - val_loss: 0.3973\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4244 - val_loss: 0.3904\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.4190 - val_loss: 0.3886\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.4164 - val_loss: 0.3810\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4110 - val_loss: 0.3808\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4078 - val_loss: 0.3783\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.4044 - val_loss: 0.3720\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4025 - val_loss: 0.3716\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3957 - val_loss: 0.3664\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3940 - val_loss: 0.3646\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3903 - val_loss: 0.3624\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3875 - val_loss: 0.3592\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3836 - val_loss: 0.3697\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3793 - val_loss: 0.5104\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3883 - val_loss: 0.3589\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3781 - val_loss: 0.3546\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.5031\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3817 - val_loss: 0.3576\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3738 - val_loss: 0.3559\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.3773 - val_loss: 0.3518\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3787 - val_loss: 0.3508\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3793 - val_loss: 0.9326\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3786 - val_loss: 0.3503\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3704 - val_loss: 0.3477\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3662 - val_loss: 0.3455\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3711 - val_loss: 0.3458\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.3659 - val_loss: 0.3450\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3652 - val_loss: 0.3433\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3654 - val_loss: 0.3429\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.3462\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3596 - val_loss: 0.3497\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3602 - val_loss: 0.3419\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3417\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3575 - val_loss: 0.3393\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3660 - val_loss: 0.3409\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3550 - val_loss: 0.3377\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3561 - val_loss: 0.3377\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 1.5096\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.3466\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3624 - val_loss: 0.3435\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3584 - val_loss: 0.3392\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3699 - val_loss: 0.3443\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3536 - val_loss: 0.3364\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3577 - val_loss: 0.3363\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3526 - val_loss: 0.3415\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3499 - val_loss: 0.3352\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3491 - val_loss: 0.3345\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3505 - val_loss: 0.3388\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3464 - val_loss: 0.3365\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3494 - val_loss: 0.3336\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3510 - val_loss: 0.3671\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3603 - val_loss: 0.3365\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3480 - val_loss: 0.3346\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3434 - val_loss: 0.3343\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3431 - val_loss: 0.3328\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3420 - val_loss: 0.3430\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3426 - val_loss: 0.3331\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.3304\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3403 - val_loss: 0.3294\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3453 - val_loss: 0.3327\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3415 - val_loss: 0.3301\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3365 - val_loss: 1.5337\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.3345\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3480 - val_loss: 0.3326\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3421 - val_loss: 0.3292\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3412 - val_loss: 0.3308\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3411 - val_loss: 0.3287\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3381 - val_loss: 0.3289\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3381 - val_loss: 0.3284\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3370 - val_loss: 0.3296\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3371 - val_loss: 0.3292\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3359 - val_loss: 0.3353\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3394 - val_loss: 0.3273\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3352 - val_loss: 0.3288\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3343 - val_loss: 0.3282\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3339 - val_loss: 0.3265\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3363 - val_loss: 0.3277\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3328 - val_loss: 0.3260\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3326 - val_loss: 0.3266\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3319 - val_loss: 0.3266\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3326 - val_loss: 0.3254\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3317 - val_loss: 0.3292\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3313 - val_loss: 0.3253\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3312 - val_loss: 0.3254\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3308 - val_loss: 0.3254\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3306 - val_loss: 0.3242\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3310 - val_loss: 0.3239\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3308 - val_loss: 0.3239\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3293 - val_loss: 0.3271\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3296 - val_loss: 0.3269\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.3230\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3304 - val_loss: 0.3225\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3291 - val_loss: 0.3261\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3285 - val_loss: 0.3233\n",
      "121/121 [==============================] - 0s 609us/step - loss: 0.3610\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3224 - val_loss: 4.0157\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.6164 - val_loss: 0.5443\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5452 - val_loss: 0.4887\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5001 - val_loss: 0.4553\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4732 - val_loss: 0.4373\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4584 - val_loss: 0.4277\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4493 - val_loss: 0.4214\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4417 - val_loss: 0.4145\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4353 - val_loss: 0.4095\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.4314 - val_loss: 0.4049\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4263 - val_loss: 0.4012\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.4218 - val_loss: 0.4022\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.4187 - val_loss: 0.3967\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.3971\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.3895\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4084 - val_loss: 0.3867\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4055 - val_loss: 0.3832\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4034 - val_loss: 0.3816\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3993 - val_loss: 0.3800\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3974 - val_loss: 0.3784\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.3758\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.3731\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3902 - val_loss: 0.3716\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3876 - val_loss: 0.3706\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3859 - val_loss: 0.3698\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3840 - val_loss: 0.3679\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.3815 - val_loss: 0.3664\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3806 - val_loss: 0.3631\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3783 - val_loss: 0.3664\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3779 - val_loss: 0.3648\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3761 - val_loss: 0.3603\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3744 - val_loss: 0.3564\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3735 - val_loss: 0.3631\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.3660\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3715 - val_loss: 0.3588\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3702 - val_loss: 0.3551\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3699 - val_loss: 0.3661\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3684 - val_loss: 0.3850\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3676 - val_loss: 0.3775\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3671 - val_loss: 0.3733\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3660 - val_loss: 0.3738\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3652 - val_loss: 0.4092\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3643 - val_loss: 0.3768\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3640 - val_loss: 0.3918\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3628 - val_loss: 0.4061\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3628 - val_loss: 0.3919\n",
      "121/121 [==============================] - 0s 599us/step - loss: 0.3742\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3569 - val_loss: 0.6912\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6746 - val_loss: 0.5962\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6001 - val_loss: 0.5387\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5444 - val_loss: 0.4937\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5045 - val_loss: 0.4592\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4799 - val_loss: 0.4389\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4626 - val_loss: 0.4238\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4524 - val_loss: 0.4126\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4437 - val_loss: 0.4055\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4380 - val_loss: 0.3996\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4340 - val_loss: 0.3967\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.4295 - val_loss: 0.3941\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.4255 - val_loss: 0.3915\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4243 - val_loss: 0.3886\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4217 - val_loss: 0.3848\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4185 - val_loss: 0.3826\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.4164 - val_loss: 0.3825\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4144 - val_loss: 0.3825\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 0.3785\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4100 - val_loss: 0.3764\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4084 - val_loss: 0.3742\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4066 - val_loss: 0.3735\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4046 - val_loss: 0.3724\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4027 - val_loss: 0.3724\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4011 - val_loss: 0.3685\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3991 - val_loss: 0.3696\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.3655\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3958 - val_loss: 0.3664\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.3662\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3945 - val_loss: 0.3627\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3913 - val_loss: 0.3632\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3901 - val_loss: 0.3597\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3885 - val_loss: 0.3609\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.3863 - val_loss: 0.3607\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3859 - val_loss: 0.3575\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3833 - val_loss: 0.3552\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3830 - val_loss: 0.3558\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3816 - val_loss: 0.3544\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3802 - val_loss: 0.3526\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.3521\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3778 - val_loss: 0.3510\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3766 - val_loss: 0.3505\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.3506\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.3480\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3745 - val_loss: 0.3487\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3734 - val_loss: 0.3471\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.3453\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.3715 - val_loss: 0.3446\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3702 - val_loss: 0.3455\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.3705 - val_loss: 0.3449\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3690 - val_loss: 0.3447\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3661 - val_loss: 0.3471\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3424\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 0.3932\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3627 - val_loss: 0.3443\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.3427\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.3733\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3651 - val_loss: 0.3404\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3634 - val_loss: 0.3402\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3618 - val_loss: 0.3398\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3618 - val_loss: 0.5376\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3662 - val_loss: 0.3401\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3647 - val_loss: 0.3384\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.3385\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.3392\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3585 - val_loss: 1.2672\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3657 - val_loss: 0.3423\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3689 - val_loss: 0.3379\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3638 - val_loss: 0.3378\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3625 - val_loss: 0.3367\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3621 - val_loss: 0.3367\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3598 - val_loss: 0.3382\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3575 - val_loss: 0.8599\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3582 - val_loss: 0.3395\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3618 - val_loss: 0.3359\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3587 - val_loss: 0.3358\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3585 - val_loss: 0.3357\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3554 - val_loss: 0.3397\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3553 - val_loss: 1.0173\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3633 - val_loss: 0.3393\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.3626 - val_loss: 0.3349\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3591 - val_loss: 0.3339\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3578 - val_loss: 0.3343\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3572 - val_loss: 0.3342\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3555 - val_loss: 1.4195\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3669 - val_loss: 0.3378\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3643 - val_loss: 0.3338\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3589 - val_loss: 0.3328\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3577 - val_loss: 0.3340\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3562 - val_loss: 0.3331\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3385\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3522 - val_loss: 0.3331\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3533 - val_loss: 0.3312\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3503 - val_loss: 0.6704\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3519 - val_loss: 0.3308\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3508 - val_loss: 0.3911\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3575 - val_loss: 0.3427\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3603 - val_loss: 0.3326\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3522 - val_loss: 0.3527\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3496 - val_loss: 0.4225\n",
      "121/121 [==============================] - 0s 672us/step - loss: 0.3481\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0446 - val_loss: 7.4231\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 3.7533 - val_loss: 5.8253\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 3.4938 - val_loss: 4.6996\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 3.2701 - val_loss: 3.9309\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 3.0755 - val_loss: 3.3967\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 2.9041 - val_loss: 3.0094\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 2.7519 - val_loss: 2.7290\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 2.6159 - val_loss: 2.5190\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 2.4881 - val_loss: 2.3540\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 2.3639 - val_loss: 2.2123\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 2.2369 - val_loss: 2.0797\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 2.1005 - val_loss: 1.9363\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 1.9501 - val_loss: 1.7796\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 1.7830 - val_loss: 1.6101\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 1.6020 - val_loss: 1.4289\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 1.4157 - val_loss: 1.2522\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 1.2450 - val_loss: 1.0966\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 1.1011 - val_loss: 0.9684\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9864 - val_loss: 0.8698\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.8997 - val_loss: 0.7974\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8360 - val_loss: 0.7451\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7896 - val_loss: 0.7076\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7559 - val_loss: 0.6800\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7309 - val_loss: 0.6597\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7122 - val_loss: 0.6442\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6977 - val_loss: 0.6322\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6863 - val_loss: 0.6227\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6772 - val_loss: 0.6149\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6697 - val_loss: 0.6085\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.6633 - val_loss: 0.6029\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.6578 - val_loss: 0.5980\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.6529 - val_loss: 0.5937\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.6484 - val_loss: 0.5897\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.6444 - val_loss: 0.5860\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.6406 - val_loss: 0.5826\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.6371 - val_loss: 0.5794\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.6338 - val_loss: 0.5763\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.6305 - val_loss: 0.5732\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.6272 - val_loss: 0.5701\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.6240 - val_loss: 0.5671\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.6208 - val_loss: 0.5642\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.6177 - val_loss: 0.5613\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.6147 - val_loss: 0.5584\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.6117 - val_loss: 0.5557\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.6088 - val_loss: 0.5530\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.6060 - val_loss: 0.5503\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.6032 - val_loss: 0.5477\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.6005 - val_loss: 0.5452\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.5979 - val_loss: 0.5428\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.5953 - val_loss: 0.5403\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.5927 - val_loss: 0.5379\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.5902 - val_loss: 0.5356\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5878 - val_loss: 0.5334\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.5854 - val_loss: 0.5311\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.5830 - val_loss: 0.5289\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.5807 - val_loss: 0.5268\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.5784 - val_loss: 0.5247\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5762 - val_loss: 0.5227\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.5740 - val_loss: 0.5207\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.5718 - val_loss: 0.5187\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.5697 - val_loss: 0.5168\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.5676 - val_loss: 0.5149\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.5655 - val_loss: 0.5131\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.5635 - val_loss: 0.5113\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.5615 - val_loss: 0.5095\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.5596 - val_loss: 0.5078\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.5577 - val_loss: 0.5061\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.5558 - val_loss: 0.5045\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5540 - val_loss: 0.5029\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.5522 - val_loss: 0.5013\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.4998\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5487 - val_loss: 0.4983\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.5470 - val_loss: 0.4968\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5453 - val_loss: 0.4953\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5435 - val_loss: 0.4938\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5419 - val_loss: 0.4924\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5402 - val_loss: 0.4909\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5385 - val_loss: 0.4895\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5369 - val_loss: 0.4881\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.4867\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5336 - val_loss: 0.4853\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.4839\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.4825\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5287 - val_loss: 0.4811\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5270 - val_loss: 0.4797\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 0.4783\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 0.4770\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.4758\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5204 - val_loss: 0.4745\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.4732\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5172 - val_loss: 0.4721\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5156 - val_loss: 0.4709\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5141 - val_loss: 0.4699\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5127 - val_loss: 0.4688\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5113 - val_loss: 0.4678\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5101 - val_loss: 0.4668\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5091 - val_loss: 0.4659\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5080 - val_loss: 0.4650\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5071 - val_loss: 0.4642\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5062 - val_loss: 0.4634\n",
      "121/121 [==============================] - 0s 811us/step - loss: 0.5044\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.0526 - val_loss: 4.9301\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9871 - val_loss: 4.8691\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9230 - val_loss: 4.8063\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8574 - val_loss: 4.7398\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7904 - val_loss: 4.6698\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7185 - val_loss: 4.5924\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6382 - val_loss: 4.5033\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5470 - val_loss: 4.3994\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4404 - val_loss: 4.2757\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3144 - val_loss: 4.1300\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1572 - val_loss: 3.9455\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9380 - val_loss: 3.6759\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6210 - val_loss: 3.3086\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1938 - val_loss: 2.8217\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6685 - val_loss: 2.2868\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1545 - val_loss: 1.8356\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7540 - val_loss: 1.5000\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4558 - val_loss: 1.2522\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2346 - val_loss: 1.0700\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0710 - val_loss: 0.9359\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.9496 - val_loss: 0.8367\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.8592 - val_loss: 0.7629\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.7915 - val_loss: 0.7079\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.7407 - val_loss: 0.6668\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.7025 - val_loss: 0.6358\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6734 - val_loss: 0.6123\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6512 - val_loss: 0.5943\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.6340 - val_loss: 0.5805\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6207 - val_loss: 0.5696\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6101 - val_loss: 0.5611\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6018 - val_loss: 0.5542\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5950 - val_loss: 0.5486\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5895 - val_loss: 0.5440\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5848 - val_loss: 0.5400\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5809 - val_loss: 0.5367\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5775 - val_loss: 0.5337\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.5745 - val_loss: 0.5311\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5718 - val_loss: 0.5287\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5694 - val_loss: 0.5265\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5672 - val_loss: 0.5245\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.5651 - val_loss: 0.5226\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5632 - val_loss: 0.5208\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5614 - val_loss: 0.5190\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5596 - val_loss: 0.5174\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.5580 - val_loss: 0.5158\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5563 - val_loss: 0.5142\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5548 - val_loss: 0.5127\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.5533 - val_loss: 0.5113\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.5518 - val_loss: 0.5099\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.5504 - val_loss: 0.5085\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5490 - val_loss: 0.5071\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.5477 - val_loss: 0.5058\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5463 - val_loss: 0.5045\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.5450 - val_loss: 0.5032\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.5437 - val_loss: 0.5020\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.5425 - val_loss: 0.5007\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.5413 - val_loss: 0.4995\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.5401 - val_loss: 0.4984\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5389 - val_loss: 0.4972\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5378 - val_loss: 0.4961\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.5367 - val_loss: 0.4950\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.5356 - val_loss: 0.4939\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5345 - val_loss: 0.4929\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.5334 - val_loss: 0.4919\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5324 - val_loss: 0.4909\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5314 - val_loss: 0.4899\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.5304 - val_loss: 0.4889\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.5294 - val_loss: 0.4879\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.5285 - val_loss: 0.4870\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5275 - val_loss: 0.4861\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.5266 - val_loss: 0.4852\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.5257 - val_loss: 0.4843\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5248 - val_loss: 0.4834\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5239 - val_loss: 0.4825\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5231 - val_loss: 0.4817\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5222 - val_loss: 0.4809\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.5214 - val_loss: 0.4801\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.5206 - val_loss: 0.4793\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.5198 - val_loss: 0.4785\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.5190 - val_loss: 0.4778\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5183 - val_loss: 0.4770\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5175 - val_loss: 0.4763\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.5168 - val_loss: 0.4756\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5160 - val_loss: 0.4749\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5153 - val_loss: 0.4742\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5146 - val_loss: 0.4735\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5139 - val_loss: 0.4728\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5132 - val_loss: 0.4722\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.5126 - val_loss: 0.4715\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5119 - val_loss: 0.4709\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.5113 - val_loss: 0.4702\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5107 - val_loss: 0.4696\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.5101 - val_loss: 0.4690\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5095 - val_loss: 0.4685\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5088 - val_loss: 0.4679\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.5083 - val_loss: 0.4673\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5077 - val_loss: 0.4667\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.5071 - val_loss: 0.4661\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5066 - val_loss: 0.4656\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.5060 - val_loss: 0.4651\n",
      "121/121 [==============================] - 0s 567us/step - loss: 0.5252\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.5143 - val_loss: 45.5993\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 5.1873 - val_loss: 31.0727\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 4.9570 - val_loss: 21.6367\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 4.7886 - val_loss: 15.5022\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 4.6544 - val_loss: 11.5021\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 4.5390 - val_loss: 8.8676\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 4.4301 - val_loss: 7.0969\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 4.3198 - val_loss: 5.8924\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 4.2002 - val_loss: 5.0697\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 4.0608 - val_loss: 4.4783\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 3.9025 - val_loss: 4.0197\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 3.7251 - val_loss: 3.6603\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 3.5205 - val_loss: 3.3419\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 3.2819 - val_loss: 3.0386\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 3.0119 - val_loss: 2.7367\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 2.6958 - val_loss: 2.4048\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 2.3396 - val_loss: 2.0563\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 1.9804 - val_loss: 1.7221\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 1.6537 - val_loss: 1.4316\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 1.3792 - val_loss: 1.1966\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 1.1653 - val_loss: 1.0205\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 1.0096 - val_loss: 0.8937\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.8992 - val_loss: 0.8031\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.8204 - val_loss: 0.7379\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.7637 - val_loss: 0.6904\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.7225 - val_loss: 0.6555\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.6922 - val_loss: 0.6295\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.6695 - val_loss: 0.6100\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.6524 - val_loss: 0.5951\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6394 - val_loss: 0.5835\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.6293 - val_loss: 0.5744\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.6212 - val_loss: 0.5669\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.6144 - val_loss: 0.5607\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.6089 - val_loss: 0.5555\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.6042 - val_loss: 0.5511\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.6001 - val_loss: 0.5473\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.5965 - val_loss: 0.5438\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.5932 - val_loss: 0.5406\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5903 - val_loss: 0.5377\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5875 - val_loss: 0.5350\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5849 - val_loss: 0.5324\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.5823 - val_loss: 0.5300\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.5799 - val_loss: 0.5278\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5777 - val_loss: 0.5256\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5754 - val_loss: 0.5234\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.5733 - val_loss: 0.5214\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.5713 - val_loss: 0.5194\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.5693 - val_loss: 0.5174\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5673 - val_loss: 0.5155\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.5655 - val_loss: 0.5137\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5637 - val_loss: 0.5120\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5620 - val_loss: 0.5103\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.5603 - val_loss: 0.5085\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.5586 - val_loss: 0.5068\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5568 - val_loss: 0.5050\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5550 - val_loss: 0.5032\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5532 - val_loss: 0.5015\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.5515 - val_loss: 0.4998\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.5498 - val_loss: 0.4980\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.5481 - val_loss: 0.4963\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.5464 - val_loss: 0.4946\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5448 - val_loss: 0.4930\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.5431 - val_loss: 0.4913\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5413 - val_loss: 0.4897\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5397 - val_loss: 0.4882\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5381 - val_loss: 0.4868\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.5366 - val_loss: 0.4854\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5350 - val_loss: 0.4840\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5333 - val_loss: 0.4827\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 0.4815\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.4803\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5286 - val_loss: 0.4790\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5272 - val_loss: 0.4777\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.5258 - val_loss: 0.4766\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 0.4755\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.4744\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.5223 - val_loss: 0.4735\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.5212 - val_loss: 0.4726\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5203 - val_loss: 0.4717\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5193 - val_loss: 0.4708\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4700\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5174 - val_loss: 0.4692\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5164 - val_loss: 0.4684\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5155 - val_loss: 0.4676\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5147 - val_loss: 0.4669\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.5139 - val_loss: 0.4662\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.5131 - val_loss: 0.4655\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.5124 - val_loss: 0.4649\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5116 - val_loss: 0.4642\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.5109 - val_loss: 0.4636\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5103 - val_loss: 0.4629\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.5096 - val_loss: 0.4623\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.5089 - val_loss: 0.4617\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.5083 - val_loss: 0.4611\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5077 - val_loss: 0.4606\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5071 - val_loss: 0.4600\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.5065 - val_loss: 0.4595\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.5059 - val_loss: 0.4589\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.5054 - val_loss: 0.4584\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5049 - val_loss: 0.4579\n",
      "121/121 [==============================] - 0s 605us/step - loss: 0.4917\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3884 - val_loss: 0.5901\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5804 - val_loss: 0.5110\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 0.4604\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4686 - val_loss: 0.4270\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4415 - val_loss: 0.4079\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4236 - val_loss: 0.3973\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4115 - val_loss: 0.3858\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4029 - val_loss: 0.3801\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.3768\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.3684\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.3673\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.3691\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3600\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.3596\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.3554\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.3548\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3662 - val_loss: 0.3528\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.3504\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.3495\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.3463\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3566 - val_loss: 0.3488\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3548 - val_loss: 0.3452\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.3422\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3510 - val_loss: 0.3444\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3427\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3410\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3461 - val_loss: 0.3370\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3446 - val_loss: 0.3376\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3425 - val_loss: 0.3352\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3415 - val_loss: 0.3338\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3404 - val_loss: 0.3322\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.3315\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.3302\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3282\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3356 - val_loss: 0.3286\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3303\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.3251\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3318 - val_loss: 0.3234\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3299 - val_loss: 0.3253\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.4740\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3306 - val_loss: 0.3252\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3284 - val_loss: 0.3218\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3272 - val_loss: 0.4576\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3268 - val_loss: 0.3211\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3297\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.3222\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3225 - val_loss: 0.4776\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3202\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3361\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3203 - val_loss: 0.3378\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3189 - val_loss: 0.3402\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3185 - val_loss: 0.3409\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3267\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3172 - val_loss: 0.3166\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.3169\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3155 - val_loss: 0.3126\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3899\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3144 - val_loss: 0.3098\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3128 - val_loss: 0.3094\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3120 - val_loss: 0.3221\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3434\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3108 - val_loss: 0.3385\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.3300\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3094 - val_loss: 0.3831\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.3109\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3085 - val_loss: 0.3181\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3075 - val_loss: 0.4024\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3077 - val_loss: 0.3061\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3060 - val_loss: 0.3855\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3072 - val_loss: 0.3073\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3050 - val_loss: 0.4777\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3066 - val_loss: 0.3063\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3037 - val_loss: 0.4677\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3043 - val_loss: 0.3048\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3036 - val_loss: 0.3126\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3013 - val_loss: 0.5283\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3037 - val_loss: 0.3053\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.3056\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3016 - val_loss: 0.3050\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3003 - val_loss: 0.5707\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3027 - val_loss: 0.3041\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3003 - val_loss: 0.3012\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 0.7652\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3038 - val_loss: 0.3029\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2995 - val_loss: 0.3003\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2978 - val_loss: 0.2999\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2971 - val_loss: 0.2992\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2961 - val_loss: 0.5283\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2977 - val_loss: 0.3001\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3005\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2958 - val_loss: 0.2997\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2946 - val_loss: 0.2993\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.2970\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2936 - val_loss: 0.2955\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2923 - val_loss: 0.3081\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2917 - val_loss: 0.3015\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2923 - val_loss: 0.2970\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2924 - val_loss: 0.2961\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2916 - val_loss: 0.2988\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2902 - val_loss: 0.3101\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3297\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2242 - val_loss: 2.2442\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5558 - val_loss: 0.6210\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4838 - val_loss: 0.4449\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4469 - val_loss: 0.4115\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4241 - val_loss: 0.3934\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.3828\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3999 - val_loss: 0.4126\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3920 - val_loss: 0.4440\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3853 - val_loss: 0.5542\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.5952\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.6810\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.7249\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3691 - val_loss: 0.7952\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.9037\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.9092\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.9228\n",
      "121/121 [==============================] - 0s 742us/step - loss: 0.3778\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3218 - val_loss: 0.6294\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5797 - val_loss: 0.4847\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4999 - val_loss: 0.4419\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4681 - val_loss: 0.4186\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4511 - val_loss: 0.4036\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4354 - val_loss: 0.3924\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.3837\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.3775\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.3728\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.3683\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4016 - val_loss: 0.3661\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3964 - val_loss: 0.3628\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3918 - val_loss: 0.3616\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.3591\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3847 - val_loss: 0.3541\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.3510\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3776 - val_loss: 0.3509\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.3507\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3723 - val_loss: 0.3463\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3695 - val_loss: 0.3440\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.3405\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.5546\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3618 - val_loss: 0.3401\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.3506\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.3352\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.5060\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3477\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.3368\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.4135\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3847\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3314\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.4340\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3443 - val_loss: 0.4880\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3276\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.4241\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.3243\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.3336\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.3931\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3237\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3746\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3370 - val_loss: 0.4214\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3349 - val_loss: 0.4326\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3223\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3343 - val_loss: 0.3203\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3332 - val_loss: 0.4246\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3321 - val_loss: 0.4046\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3301 - val_loss: 0.3204\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3316 - val_loss: 0.3180\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 1.1839\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3198\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3179\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.3182\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.4653\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3274 - val_loss: 0.3202\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3268 - val_loss: 0.3174\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3139\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.6270\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.3148\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3238 - val_loss: 0.3167\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.6557\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3252 - val_loss: 0.3130\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.3119\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 1.1910\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.3127\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3218 - val_loss: 0.3116\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.3124\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3183 - val_loss: 0.4612\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3104\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3170 - val_loss: 0.7129\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.3092\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3082\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3156 - val_loss: 0.3090\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3153 - val_loss: 0.3089\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3069\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3133 - val_loss: 0.3068\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3129 - val_loss: 0.3052\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3146 - val_loss: 0.3055\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3105 - val_loss: 0.3061\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3103 - val_loss: 1.3229\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3172 - val_loss: 0.3083\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3051\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3123 - val_loss: 0.3030\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3102 - val_loss: 0.3041\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3112 - val_loss: 0.3051\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3087\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3109 - val_loss: 0.8306\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3289 - val_loss: 0.3018\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3101 - val_loss: 0.3016\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3097 - val_loss: 0.3082\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3055 - val_loss: 0.3025\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3081 - val_loss: 0.5441\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.3069\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3094 - val_loss: 0.3026\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3084 - val_loss: 0.3048\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.2997\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3034 - val_loss: 0.3015\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3271\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3079\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3150 - val_loss: 0.3009\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3180 - val_loss: 0.3008\n",
      "121/121 [==============================] - 0s 693us/step - loss: 0.3150\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.1268 - val_loss: 0.7037\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 948us/step - loss: 0.5796 - val_loss: 0.5010\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4923 - val_loss: 0.4383\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 988us/step - loss: 0.4459 - val_loss: 0.4171\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.4207 - val_loss: 0.3887\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 940us/step - loss: 0.4049 - val_loss: 0.5024\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 955us/step - loss: 0.3978 - val_loss: 0.3692\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3869 - val_loss: 0.3702\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3804 - val_loss: 0.3621\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3743 - val_loss: 0.4633\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 931us/step - loss: 0.3726 - val_loss: 0.3524\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 957us/step - loss: 0.3694 - val_loss: 0.3508\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3638 - val_loss: 0.4697\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.3623 - val_loss: 0.3454\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 989us/step - loss: 0.3597 - val_loss: 0.6232\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3418\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 970us/step - loss: 0.3569 - val_loss: 0.3396\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.3377\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.3523 - val_loss: 0.3703\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3593\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3354\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.5534\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3472\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3335\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3306\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3425 - val_loss: 0.4627\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3431 - val_loss: 0.3318\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.3308\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 940us/step - loss: 0.3410 - val_loss: 0.4666\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 957us/step - loss: 0.3401 - val_loss: 0.3305\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 974us/step - loss: 0.3383 - val_loss: 0.3698\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3364 - val_loss: 0.3270\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3362 - val_loss: 0.3563\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3257\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3346 - val_loss: 0.3242\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 938us/step - loss: 0.3338 - val_loss: 0.3228\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.3321 - val_loss: 0.4128\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3314 - val_loss: 0.3234\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.4729\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3326 - val_loss: 0.3254\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3318 - val_loss: 0.3213\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 963us/step - loss: 0.3294 - val_loss: 0.4757\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 955us/step - loss: 0.3302 - val_loss: 0.3247\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3183\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.3261 - val_loss: 0.3175\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3252 - val_loss: 0.3212\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3256 - val_loss: 0.4720\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 0.3181\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3180\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3227 - val_loss: 0.3182\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3201\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.3197\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.3208 - val_loss: 0.4700\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3146\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.3212 - val_loss: 0.3138\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 944us/step - loss: 0.3202 - val_loss: 0.4797\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 961us/step - loss: 0.3205 - val_loss: 0.3117\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.3189 - val_loss: 0.3207\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 920us/step - loss: 0.3182 - val_loss: 0.3114\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.3164 - val_loss: 0.4051\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 934us/step - loss: 0.3165 - val_loss: 0.3173\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 965us/step - loss: 0.3163 - val_loss: 0.3108\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 955us/step - loss: 0.3166 - val_loss: 0.4520\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 946us/step - loss: 0.3176 - val_loss: 0.3093\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.3146 - val_loss: 0.3090\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 971us/step - loss: 0.3129 - val_loss: 0.3082\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 927us/step - loss: 0.3125 - val_loss: 0.4557\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3130 - val_loss: 0.3070\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3122 - val_loss: 0.3078\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 976us/step - loss: 0.3108 - val_loss: 0.3057\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3102 - val_loss: 0.5337\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.3101 - val_loss: 0.3055\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3111 - val_loss: 0.3035\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.3082 - val_loss: 0.3053\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3063 - val_loss: 0.4263\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.3069 - val_loss: 0.3024\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.3073 - val_loss: 0.3043\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 982us/step - loss: 0.3049 - val_loss: 0.4377\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.3101 - val_loss: 0.3029\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.3064 - val_loss: 0.3002\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 979us/step - loss: 0.3033 - val_loss: 0.3132\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 982us/step - loss: 0.3018 - val_loss: 0.2993\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.3022 - val_loss: 0.6247\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3075 - val_loss: 0.2991\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3021 - val_loss: 0.3018\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.3014 - val_loss: 0.2999\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 976us/step - loss: 0.3010 - val_loss: 0.2992\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 982us/step - loss: 0.2991 - val_loss: 0.3646\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.3003 - val_loss: 0.2942\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.2988 - val_loss: 0.2944\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 957us/step - loss: 0.2982 - val_loss: 0.4285\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2993 - val_loss: 0.2947\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.2977 - val_loss: 0.2939\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2968 - val_loss: 0.2937\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2954 - val_loss: 0.4541\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 998us/step - loss: 0.2957 - val_loss: 0.2934\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 930us/step - loss: 0.2946 - val_loss: 0.2958\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.2946 - val_loss: 0.6199\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.2949 - val_loss: 0.2932\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.2943 - val_loss: 0.2889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7fc0beabe2e0>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc0ac00eb20>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the lib\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# create the hyper-parameter distribution\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "# create RandomizedSearchCV object\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "\n",
    "# fit \n",
    "rnd_search_cv.fit(X_train_scaled, y_train, \n",
    "                 epochs=100, \n",
    "                 validation_data=(X_valid_scaled, y_valid), \n",
    "                 callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fe485",
   "metadata": {},
   "source": [
    "### show the selected model with the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b5f573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c510942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.34079789121945697"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26adf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = rnd_search_cv.best_estimator_.model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3a004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
